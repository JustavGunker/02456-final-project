{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3f9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd # Added for nice tables\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "\n",
    "# --- 1. Project Setup ---\n",
    "current_path = Path.cwd()\n",
    "PROJECT_ROOT = None\n",
    "for parent in [current_path] + list(current_path.parents):\n",
    "    if (parent / \"func\").exists():\n",
    "        PROJECT_ROOT = parent\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\"Could not find project root containing 'func' folder.\")\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from func.Models import VAE, MultiTaskNet_ag, MultiTaskNet_big\n",
    "from func.dataloaders import VolumetricPatchDataset \n",
    "\n",
    "# --- 2. Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_SHAPE = (128, 128, 128)\n",
    "NUM_CLASSES = 4\n",
    "LATENT_DIM = 256\n",
    "\n",
    "# Define your Model Paths\n",
    "PATHS = {\n",
    "    \"VAE\": PROJECT_ROOT / \"Trained_models\" / \"VAE_val_best.pth\",\n",
    "    \"AG_Net\": PROJECT_ROOT / \"Trained_models\" / \"AG_val_best.pth\",\n",
    "    \"Multi_Big\": PROJECT_ROOT / \"Trained_models\" / \"multi_big_best.pth\"\n",
    "}\n",
    "\n",
    "TEST_COLS = [35, 36, 37, 38] \n",
    "\n",
    "# --- 3. Metric Functions ---\n",
    "\n",
    "def calculate_iou_per_class(pred, target, num_classes):\n",
    "    \"\"\"Calculates IoU for each class in the 3D volume.\"\"\"\n",
    "    ious = []\n",
    "    # Flatten tensors for easier processing\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        \n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan')) # Ignore if class not present\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "            \n",
    "    return np.array(ious)\n",
    "\n",
    "def calculate_auroc(logits, target, num_classes):\n",
    "    \"\"\"\n",
    "    Calculates One-vs-Rest AUROC for multi-class segmentation.\n",
    "    NOTE: This flattens the 3D volume, which can be heavy.\n",
    "    \"\"\"\n",
    "    # logits: (1, C, D, H, W) -> Softmax -> Permute -> Flatten\n",
    "    probs = F.softmax(logits, dim=1).squeeze(0) # (C, D, H, W)\n",
    "    probs = probs.permute(1, 2, 3, 0).reshape(-1, num_classes).cpu().numpy()\n",
    "    \n",
    "    # target: (1, D, H, W) -> Flatten\n",
    "    target = target.squeeze(0).reshape(-1).cpu().numpy()\n",
    "    \n",
    "    # Calculate weighted AUROC (One-vs-Rest)\n",
    "    try:\n",
    "        score = roc_auc_score(target, probs, multi_class='ovr', average='weighted')\n",
    "        return score\n",
    "    except ValueError:\n",
    "        return 0.0 # Handle cases where not all classes are present in the patch\n",
    "\n",
    "def calculate_reconstruction_mse(recon, input_img):\n",
    "    \"\"\"Calculates Mean Squared Error for reconstruction.\"\"\"\n",
    "    recon_flat = recon.view(-1).cpu().numpy()\n",
    "    input_flat = input_img.view(-1).cpu().numpy()\n",
    "    return mean_squared_error(input_flat, recon_flat)\n",
    "\n",
    "# --- 4. Model Loading Helper ---\n",
    "def load_model(model_class, path, model_name):\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    try:\n",
    "        # Check init signature based on class type\n",
    "        if model_name == \"VAE\":\n",
    "            model = model_class(in_channels=1, latent_dim=LATENT_DIM, NUM_CLASSES=NUM_CLASSES)\n",
    "        else:\n",
    "            model = model_class(in_channels=1, latent_dim=LATENT_DIM, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        model.to(DEVICE)\n",
    "        if not path.exists():\n",
    "            print(f\"  ❌ Warning: File not found at {path}\")\n",
    "            return None\n",
    "        \n",
    "        # Safe loading\n",
    "        state_dict = torch.load(path, map_location=DEVICE)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error loading {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 5. Main Execution ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "845b1dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Data...\n",
      "Loading VAE...\n",
      "Loading AG_Net...\n",
      "Loading Multi_Big...\n",
      "\n",
      "Running Inference & Metrics Calculation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1416073/4114043073.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=DEVICE)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Handle output formats\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) == \u001b[32m4\u001b[39m: seg_logits, recon, _, _ = output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/MBML/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/MBML/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Deep_project/02456-final-project/func/Models.py:546\u001b[39m, in \u001b[36mVAE.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    544\u001b[39m logvar = \u001b[38;5;28mself\u001b[39m.z_logvar(b)\n\u001b[32m    545\u001b[39m z = \u001b[38;5;28mself\u001b[39m.reparameterize(mu, logvar)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m seg_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder_seg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m recon_output = \u001b[38;5;28mself\u001b[39m.decoder_recon(z, s1, s2, s3)\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m seg_output, recon_output, mu, logvar\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/MBML/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/MBML/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Deep_project/02456-final-project/func/Models.py:216\u001b[39m, in \u001b[36mSeg_decoder_big.forward\u001b[39m\u001b[34m(self, b, s1, s2, s3)\u001b[39m\n\u001b[32m    213\u001b[39m us2 = \u001b[38;5;28mself\u001b[39m.up_seg2(ds1) \u001b[38;5;66;03m# 16x16x16\u001b[39;00m\n\u001b[32m    214\u001b[39m ds2 = \u001b[38;5;28mself\u001b[39m.dec_seg2(torch.cat([us2, s2], dim=\u001b[32m1\u001b[39m)) \u001b[38;5;66;03m# cat [16x16x16] w[16x16x16]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m us3 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_seg3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 32x32x32\u001b[39;00m\n\u001b[32m    217\u001b[39m ds3 = \u001b[38;5;28mself\u001b[39m.dec_seg3(torch.cat([us3, s1], dim=\u001b[32m1\u001b[39m)) \u001b[38;5;66;03m# cat [32x32x32] w [32x32x32]\u001b[39;00m\n\u001b[32m    218\u001b[39m us4 = \u001b[38;5;28mself\u001b[39m.out_seg(ds3) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/MBML/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/MBML/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/MBML/lib/python3.12/site-packages/torch/nn/modules/conv.py:1100\u001b[39m, in \u001b[36mConvTranspose3d.forward\u001b[39m\u001b[34m(self, input, output_size)\u001b[39m\n\u001b[32m   1095\u001b[39m num_spatial_dims = \u001b[32m3\u001b[39m\n\u001b[32m   1096\u001b[39m output_padding = \u001b[38;5;28mself\u001b[39m._output_padding(\n\u001b[32m   1097\u001b[39m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m.stride, \u001b[38;5;28mself\u001b[39m.padding, \u001b[38;5;28mself\u001b[39m.kernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1098\u001b[39m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m.dilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv_transpose3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# A. Load Data\n",
    "print(\"Loading Test Data...\")\n",
    "try:\n",
    "    ds = VolumetricPatchDataset(selected_columns=TEST_COLS, augment=False, is_labeled=True)\n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "    x_batch, y_batch = next(iter(dl))\n",
    "    x_batch = x_batch.to(DEVICE)\n",
    "    y_batch = y_batch.squeeze(1).to(DEVICE) # Ensure shape (1, D, H, W)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# B. Load Models\n",
    "models = {\n",
    "    \"VAE\": load_model(VAE, PATHS[\"VAE\"], \"VAE\"),\n",
    "    \"AG_Net\": load_model(MultiTaskNet_ag, PATHS[\"AG_Net\"], \"AG_Net\"),\n",
    "    \"Multi_Big\": load_model(MultiTaskNet_big, PATHS[\"Multi_Big\"], \"Multi_Big\")\n",
    "}\n",
    "\n",
    "# C. Run Inference & Calculate Metrics\n",
    "metrics_data = []\n",
    "results = {}\n",
    "\n",
    "print(\"\\nRunning Inference & Metrics Calculation...\")\n",
    "with torch.no_grad():\n",
    "    for name, model in models.items():\n",
    "        if model is None: continue\n",
    "        \n",
    "        # Forward Pass\n",
    "        output = model(x_batch)\n",
    "        \n",
    "        # Handle output formats\n",
    "        if len(output) == 4: seg_logits, recon, _, _ = output\n",
    "        elif len(output) == 3: seg_logits, recon, _ = output\n",
    "        else: seg_logits, recon = output[0], output[1]\n",
    "        \n",
    "        # Save for plotting\n",
    "        results[name] = {\"seg\": seg_logits, \"recon\": recon}\n",
    "        \n",
    "        # --- METRICS ---\n",
    "        # 1. Segmentation Prediction (Argmax)\n",
    "        pred_seg = torch.argmax(seg_logits, dim=1)\n",
    "        \n",
    "        # 2. IoU (Per Class & Mean Foreground)\n",
    "        ious = calculate_iou_per_class(pred_seg, y_batch, NUM_CLASSES)\n",
    "        mIoU = np.nanmean(ious[1:]) # Mean of Classes 1, 2, 3 (exclude background)\n",
    "        \n",
    "        # 3. AUROC (Segmentation Performance)\n",
    "        # Note: Using only a subset of pixels if memory is tight, but here we use full patch\n",
    "        auroc = calculate_auroc(seg_logits, y_batch, NUM_CLASSES)\n",
    "        \n",
    "        # 4. ME (Reconstruction MSE)\n",
    "        mse_recon = calculate_reconstruction_mse(recon, x_batch)\n",
    "        \n",
    "        # Store Data\n",
    "        metrics_data.append({\n",
    "            \"Model\": name,\n",
    "            \"mIoU (Foreground)\": mIoU,\n",
    "            \"AUROC\": auroc,\n",
    "            \"Recon MSE (ME)\": mse_recon,\n",
    "            \"IoU Class 1\": ious[1],\n",
    "            \"IoU Class 2\": ious[2],\n",
    "            \"IoU Class 3\": ious[3]\n",
    "        })\n",
    "\n",
    "# D. Display Metrics\n",
    "df = pd.DataFrame(metrics_data)\n",
    "print(\"\\n--- Model Performance Comparison ---\")\n",
    "display(df) # Uses Jupyter's nice table display\n",
    "\n",
    "# E. Visualization (Central Slice)\n",
    "slc = 64\n",
    "img_in = x_batch[0, 0, slc].cpu().numpy()\n",
    "img_gt = y_batch[0, slc].cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1 + len(results), figsize=(4 * (1 + len(results)), 12))\n",
    "\n",
    "# Column 0: Inputs\n",
    "axes[0, 0].imshow(img_in, cmap='gray')\n",
    "axes[0, 0].set_title(\"Input Image\")\n",
    "axes[1, 0].imshow(img_gt, cmap='viridis', vmin=0, vmax=NUM_CLASSES-1)\n",
    "axes[1, 0].set_title(\"Ground Truth\")\n",
    "axes[2, 0].axis('off')\n",
    "\n",
    "# Columns 1..N: Models\n",
    "for i, (name, res) in enumerate(results.items(), start=1):\n",
    "    # Recon\n",
    "    recon_slice = res[\"recon\"][0, 0, slc].cpu().numpy()\n",
    "    axes[0, i].imshow(recon_slice, cmap='gray')\n",
    "    axes[0, i].set_title(f\"{name}\\nRecon\")\n",
    "    \n",
    "    # Segmentation\n",
    "    seg_slice = torch.argmax(res[\"seg\"], dim=1)[0, slc].cpu().numpy()\n",
    "    axes[1, i].imshow(seg_slice, cmap='viridis', vmin=0, vmax=NUM_CLASSES-1)\n",
    "    axes[1, i].set_title(f\"{name}\\nSeg Prediction\")\n",
    "    \n",
    "    # Error Map\n",
    "    diff = (seg_slice != img_gt)\n",
    "    axes[2, i].imshow(diff, cmap='Reds', vmin=0, vmax=1)\n",
    "    axes[2, i].set_title(f\"{name}\\nError Map\")\n",
    "\n",
    "for ax in axes.flatten(): ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f91044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root set to: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "\n",
    "# --- Project Path Setup ---\n",
    "# Assuming this notebook is located in 'Model/Notebooks/' or similar inside your project\n",
    "# We go up two levels to find the root. Adjust .parent count if needed.\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project Root set to: {PROJECT_ROOT}\")\n",
    "\n",
    "# --- Import Model ---\n",
    "# Change this import if you want to use MultiTaskNet_big instead\n",
    "from func.Models import MultiTaskNet_ag as MultiTaskNet \n",
    "from func.Models import VAE\n",
    "# from func.Models import MultiTaskNet_big as MultiTaskNet \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b79479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "--- Using device: cpu ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "\n",
    "# --- SKLEARN for Metrics ---\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "except ImportError:\n",
    "    print(\"WARNING: scikit-learn not found. AUROC calculation will be skipped.\")\n",
    "    roc_auc_score = None\n",
    "\n",
    "# --- Project Setup ---\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import your model\n",
    "from func.Models import MultiTaskNet_ag as MultiTaskNet \n",
    "from func.Models import VAE\n",
    "# ---------------------\n",
    "\n",
    "# --- Configuration ---\n",
    "NUM_CLASSES = 4\n",
    "LATENT_DIM = 512\n",
    "FULL_VOLUME_SIZE = 256\n",
    "PATCH_SIZE = 128\n",
    "SLICES_PER_AXIS = FULL_VOLUME_SIZE // PATCH_SIZE \n",
    "\n",
    "# Weights Path (Updated to your AG model)\n",
    "MODEL_PATH = PROJECT_ROOT / \"Trained_models\" / \"AG_val_best.pth\"\n",
    "SAVE_FILENAME = \"full_ag_best_col36.png\"\n",
    "\n",
    "# Data Directory\n",
    "BLACKHOLE_PATH = os.environ.get('BLACKHOLE', '.')\n",
    "BASE_DATA_DIR = Path(os.path.join(BLACKHOLE_PATH, 'deep_learning_214776', 'extracted_datasets', 'datasets_processed_latest'))\n",
    "\n",
    "# Test Column (e.g., 35)\n",
    "TEST_COLUMN_ID = 36\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Using device: {device} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_colormap():\n",
    "    \"\"\"\n",
    "    0: Blue (Background)\n",
    "    1: Red\n",
    "    2: Yellow\n",
    "    3: Turquoise (Cyan)\n",
    "    \"\"\"\n",
    "    colors = ['blue', 'red', 'yellow', 'cyan']\n",
    "    return ListedColormap(colors)\n",
    "\n",
    "\n",
    "def load_raw_volume(column_num, half_type='top'):\n",
    "    \"\"\"Loads the FULL 256^3 volume from disk.\"\"\"\n",
    "    column_dir = BASE_DATA_DIR / f'Column_{column_num}'\n",
    "    x_filepath = column_dir / 'B' / f'{half_type}.mat'\n",
    "    y_filepath = column_dir / f'gt_{half_type}.mat'\n",
    "    \n",
    "    print(f\"Loading: {x_filepath}\")\n",
    "    try:\n",
    "        X_full = np.squeeze(loadmat(str(x_filepath))[half_type]).astype(np.float32) / 255.0\n",
    "        Y_full = np.squeeze(loadmat(str(y_filepath))[f'gt_{half_type}']).astype(np.int64)\n",
    "        \n",
    "        X_tensor = torch.from_numpy(X_full).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        Y_tensor = torch.from_numpy(Y_full).unsqueeze(0).to(device)\n",
    "        return X_tensor, Y_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading raw volume: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def stitch_inference(model, X_full):\n",
    "    \"\"\"\n",
    "    Cuts the 256^3 volume into 8 patches, runs inference, and stitches them back.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    stitched_seg = torch.zeros((1, NUM_CLASSES, FULL_VOLUME_SIZE, FULL_VOLUME_SIZE, FULL_VOLUME_SIZE), device=device)\n",
    "    stitched_recon = torch.zeros((1, 1, FULL_VOLUME_SIZE, FULL_VOLUME_SIZE, FULL_VOLUME_SIZE), device=device)\n",
    "    \n",
    "    print(\"Starting Stitching Loop...\")\n",
    "    for pd, ph, pw in itertools.product(range(SLICES_PER_AXIS), repeat=3):\n",
    "        d_start, d_end = pd * PATCH_SIZE, (pd + 1) * PATCH_SIZE\n",
    "        h_start, h_end = ph * PATCH_SIZE, (ph + 1) * PATCH_SIZE\n",
    "        w_start, w_end = pw * PATCH_SIZE, (pw + 1) * PATCH_SIZE\n",
    "        \n",
    "        X_patch = X_full[:, :, d_start:d_end, h_start:h_end, w_start:w_end]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            seg_patch, recon_patch = model(X_patch)\n",
    "            \n",
    "        stitched_seg[:, :, d_start:d_end, h_start:h_end, w_start:w_end] = seg_patch\n",
    "        stitched_recon[:, :, d_start:d_end, h_start:h_end, w_start:w_end] = recon_patch\n",
    "        \n",
    "        print(f\"  Processed Patch: [{d_start}:{d_end}, {h_start}:{h_end}, {w_start}:{w_end}]\")\n",
    "\n",
    "    return stitched_seg, stitched_recon\n",
    "\n",
    "\n",
    "def calculate_metrics(stitched_logits, Y_gt):\n",
    "    \"\"\"Calculates IoU, ME, and AUROC.\"\"\"\n",
    "    print(\"\\n--- Calculating Full Volume Metrics ---\")\n",
    "    probs = torch.softmax(stitched_logits, dim=1)\n",
    "    preds = torch.argmax(stitched_logits, dim=1)\n",
    "    \n",
    "    y_true_flat = Y_gt.cpu().numpy().flatten()\n",
    "    y_pred_flat = preds.cpu().numpy().flatten()\n",
    "    \n",
    "    # Mean Error\n",
    "    me = 1.0 - np.mean(y_true_flat == y_pred_flat)\n",
    "    \n",
    "    # IoU\n",
    "    class_ious = []\n",
    "    print(f\"\\n[Intersection over Union]\")\n",
    "    for c in range(NUM_CLASSES):\n",
    "        intersection = np.sum((y_true_flat == c) & (y_pred_flat == c))\n",
    "        union = np.sum((y_true_flat == c) | (y_pred_flat == c))\n",
    "        \n",
    "        if union == 0:\n",
    "            iou = 1.0 if intersection == 0 else 0.0\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        print(f\"  Class {c}: {iou:.4f}\")\n",
    "        class_ious.append(iou)\n",
    "        \n",
    "    mIoU_all = np.mean(class_ious)\n",
    "    mIoU_fg = np.mean(class_ious[1:]) \n",
    "    \n",
    "    # AUROC\n",
    "    auroc = 0.0\n",
    "    if roc_auc_score is not None:\n",
    "        print(\"\\n[AUROC Calculation...]\")\n",
    "        try:\n",
    "            y_probs = probs.cpu().numpy()[0] # (C, D, H, W)\n",
    "            n_classes = y_probs.shape[0]\n",
    "            # Flatten to (N_voxels, C)\n",
    "            y_probs_flat = np.transpose(y_probs, (1, 2, 3, 0)).reshape(-1, n_classes)\n",
    "            auroc = roc_auc_score(y_true_flat, y_probs_flat, multi_class='ovr', average='macro')\n",
    "        except Exception as e:\n",
    "            print(f\"  AUROC Error: {e}\")\n",
    "            auroc = -1.0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"FINAL METRICS for Column {TEST_COLUMN_ID}\")\n",
    "    print(f\"  Mean Error (ME):       {me:.4f}\")\n",
    "    print(f\"  Mean IoU (All):        {mIoU_all:.4f}\")\n",
    "    print(f\"  Mean IoU (Foreground): {mIoU_fg:.4f}\")\n",
    "    print(f\"  AUROC (Macro):         {auroc:.4f}\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "\n",
    "def visualize_full_slice(X_tensor, Y_tensor, stitched_seg, stitched_recon, save_path, slice_idx=FULL_VOLUME_SIZE // 2):\n",
    "    \"\"\"Saves a central slice using the CUSTOM COLORMAP.\"\"\"\n",
    "    pred_seg_tensor = torch.argmax(stitched_seg, dim=1) \n",
    "    \n",
    "    x_np = X_tensor[0, 0, slice_idx, :, :].cpu().numpy()\n",
    "    y_np = Y_tensor[0, slice_idx, :, :].cpu().numpy()\n",
    "    recon_np = stitched_recon[0, 0, slice_idx, :, :].cpu().numpy()\n",
    "    pred_np = pred_seg_tensor[0, slice_idx, :, :].cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "    titles = ['Input', 'Ground Truth', 'Reconstruction', 'Prediction']\n",
    "    data = [x_np, y_np, recon_np, pred_np]\n",
    "    \n",
    "    # Use custom map for GT and Pred\n",
    "    custom_cmap = get_custom_colormap()\n",
    "    cmaps = ['gray', custom_cmap, 'gray', custom_cmap]\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        # Vmax=3 for the 4 classes (0,1,2,3)\n",
    "        vmax = NUM_CLASSES - 1 if i == 1 or i == 3 else None\n",
    "        vmin = 0 if i == 1 or i == 3 else None\n",
    "        interp = 'nearest' if i == 1 or i == 3 else None\n",
    "        \n",
    "        im = ax.imshow(data[i], cmap=cmaps[i], interpolation=interp, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(titles[i], fontsize=14)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        if i == 1 or i == 3:\n",
    "             cbar = plt.colorbar(im, ax=ax, ticks=range(NUM_CLASSES), fraction=0.046, pad=0.04)\n",
    "             cbar.ax.set_yticklabels(['Water', 'Oil', 'Solids', 'Gas'])\n",
    "\n",
    "    plt.suptitle(f\"Full Resolution Inference - Test Column {TEST_COLUMN_ID} - Slice {slice_idx}\", fontsize=16)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(f\"✅ Visualization saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738cf7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/Trained_models/AG_val_best.pth\n",
      "Loading: /dtu/blackhole/1b/167803/deep_learning_214776/extracted_datasets/datasets_processed_latest/Column_36/B/top.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996764/122819279.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stitching Loop...\n",
      "  Processed Patch: [0:128, 0:128, 0:128]\n",
      "  Processed Patch: [0:128, 0:128, 128:256]\n",
      "  Processed Patch: [0:128, 128:256, 0:128]\n",
      "  Processed Patch: [0:128, 128:256, 128:256]\n",
      "  Processed Patch: [128:256, 0:128, 0:128]\n",
      "  Processed Patch: [128:256, 0:128, 128:256]\n",
      "  Processed Patch: [128:256, 128:256, 0:128]\n",
      "  Processed Patch: [128:256, 128:256, 128:256]\n",
      "\n",
      "--- Calculating Full Volume Metrics ---\n",
      "\n",
      "[Intersection over Union]\n",
      "  Class 0: 0.7987\n",
      "  Class 1: 0.9055\n",
      "  Class 2: 0.8821\n",
      "  Class 3: 0.5043\n",
      "\n",
      "[AUROC Calculation...]\n",
      "\n",
      "==============================\n",
      "FINAL METRICS for Column 36\n",
      "  Mean Error (ME):       0.0888\n",
      "  Mean IoU (All):        0.7726\n",
      "  Mean IoU (Foreground): 0.7639\n",
      "  AUROC (Macro):         0.9814\n",
      "==============================\n",
      "\n",
      "✅ Visualization saved to: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/scripts/full_ag_best_col36.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Load Model\n",
    "    model = MultiTaskNet(in_channels=1, num_classes=NUM_CLASSES, latent_dim=LATENT_DIM).to(device)\n",
    "    #model = VAE(in_channels=1, latent_dim=LATENT_DIM, NUM_CLASSES=NUM_CLASSES)\n",
    "\n",
    "    if MODEL_PATH.exists():\n",
    "        print(f\"Loading weights from: {MODEL_PATH}\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    else:\n",
    "        print(f\"❌ Model weights not found at {MODEL_PATH}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # 2. Load Data (Column 35, Top Half)\n",
    "    X_full_tensor, Y_full_tensor = load_raw_volume(column_num=TEST_COLUMN_ID, half_type='top')\n",
    "    \n",
    "    # 3. Stitching Inference\n",
    "    stitched_seg, stitched_recon = stitch_inference(model, X_full_tensor)\n",
    "\n",
    "    # 4. Metrics Calculation\n",
    "    calculate_metrics(stitched_seg, Y_full_tensor)\n",
    "\n",
    "    # 5. Visualization\n",
    "    save_full_path = Path.cwd() / SAVE_FILENAME\n",
    "    visualize_full_slice(X_full_tensor, Y_full_tensor, stitched_seg, stitched_recon, save_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/Trained_models/VAE_val_final.pth\n",
      "Loading: /dtu/blackhole/1b/167803/deep_learning_214776/extracted_datasets/datasets_processed_latest/Column_36/B/top.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2054469/2148863801.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stitching Loop...\n",
      "  Processed Patch: [0:128, 0:128, 0:128]\n",
      "  Processed Patch: [0:128, 0:128, 128:256]\n",
      "  Processed Patch: [0:128, 128:256, 0:128]\n",
      "  Processed Patch: [0:128, 128:256, 128:256]\n",
      "  Processed Patch: [128:256, 0:128, 0:128]\n",
      "  Processed Patch: [128:256, 0:128, 128:256]\n",
      "  Processed Patch: [128:256, 128:256, 0:128]\n",
      "  Processed Patch: [128:256, 128:256, 128:256]\n",
      "\n",
      "--- Calculating Full Volume Metrics ---\n",
      "\n",
      "[Intersection over Union]\n",
      "  Class 0: 0.7878\n",
      "  Class 1: 0.9043\n",
      "  Class 2: 0.8575\n",
      "  Class 3: 0.3947\n",
      "\n",
      "[AUROC Calculation - This may take a moment for 16M voxels...]\n",
      "\n",
      "==============================\n",
      "FINAL METRICS for Column 36\n",
      "  Mean Error (ME):       0.1118\n",
      "  Mean IoU (All):        0.7361\n",
      "  Mean IoU (Foreground): 0.7188\n",
      "  AUROC (Macro):         0.9778\n",
      "==============================\n",
      "\n",
      "✅ Visualization saved to: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/scripts/full_vae_best_col36.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Load Model\n",
    "    #model = MultiTaskNet(in_channels=1, num_classes=NUM_CLASSES, latent_dim=LATENT_DIM).to(device)\n",
    "    model = VAE(in_channels=1, latent_dim=LATENT_DIM, NUM_CLASSES=NUM_CLASSES)\n",
    "\n",
    "    if MODEL_PATH.exists():\n",
    "        print(f\"Loading weights from: {MODEL_PATH}\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    else:\n",
    "        print(f\"❌ Model weights not found at {MODEL_PATH}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # 2. Load Data (Column 35, Top Half)\n",
    "    X_full_tensor, Y_full_tensor = load_raw_volume(column_num=TEST_COLUMN_ID, half_type='top')\n",
    "    \n",
    "    # 3. Stitching Inference\n",
    "    stitched_seg, stitched_recon = stitch_inference(model, X_full_tensor)\n",
    "\n",
    "    # 4. Metrics Calculation\n",
    "    calculate_metrics(stitched_seg, Y_full_tensor)\n",
    "\n",
    "    # 5. Visualization\n",
    "    save_full_path = Path.cwd() / SAVE_FILENAME\n",
    "    visualize_full_slice(X_full_tensor, Y_full_tensor, stitched_seg, stitched_recon, save_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aee8cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Visualization saved to: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/scripts/full_vae_best_col1.png\n"
     ]
    }
   ],
   "source": [
    " visualize_full_slice(X_full_tensor, Y_full_tensor, stitched_seg, stitched_recon, save_full_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MBML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
