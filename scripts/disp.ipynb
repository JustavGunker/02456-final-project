{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "--- Using device: cpu ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using device: cpu ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "\n",
    "# --- SKLEARN for Metrics ---\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "except ImportError:\n",
    "    print(\"WARNING: scikit-learn not found. AUROC calculation will be skipped.\")\n",
    "    roc_auc_score = None\n",
    "\n",
    "# --- Project Setup ---\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import your model\n",
    "from func.Models import MultiTaskNet_ag as MultiTaskNet \n",
    "from func.Models import VAE\n",
    "# ---------------------\n",
    "\n",
    "# --- Configuration ---\n",
    "NUM_CLASSES = 4\n",
    "LATENT_DIM = 512\n",
    "FULL_VOLUME_SIZE = 256\n",
    "PATCH_SIZE = 128\n",
    "SLICES_PER_AXIS = FULL_VOLUME_SIZE // PATCH_SIZE \n",
    "\n",
    "# Weights Path (Updated to your AG model)\n",
    "MODEL_PATH = PROJECT_ROOT / \"Trained_models\" / \"VAE_val_best.pth\"\n",
    "\n",
    "# Data Directory\n",
    "BLACKHOLE_PATH = os.environ.get('BLACKHOLE', '.')\n",
    "BASE_DATA_DIR = Path(os.path.join(BLACKHOLE_PATH, 'deep_learning_214776', 'extracted_datasets', 'datasets_processed_latest'))\n",
    "\n",
    "# Test Column (e.g., 35)\n",
    "TEST_COLUMN_ID = np.array([1, 2, 37,38])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Using device: {device} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aeb2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_colormap():\n",
    "    \"\"\"\n",
    "    0: Blue (Background)\n",
    "    1: Red\n",
    "    2: Yellow\n",
    "    3: Turquoise (Cyan)\n",
    "    \"\"\"\n",
    "    colors = ['blue', 'red', 'yellow', 'cyan']\n",
    "    return ListedColormap(colors)\n",
    "\n",
    "\n",
    "def load_raw_volume(column_num, half_type='top'):\n",
    "    \"\"\"Loads the FULL 256^3 volume from disk.\"\"\"\n",
    "    column_dir = BASE_DATA_DIR / f'Column_{column_num}'\n",
    "    x_filepath = column_dir / 'B' / f'{half_type}.mat'\n",
    "    y_filepath = column_dir / f'gt_{half_type}.mat'\n",
    "    \n",
    "    print(f\"Loading: {x_filepath}\")\n",
    "    try:\n",
    "        X_full = np.squeeze(loadmat(str(x_filepath))[half_type]).astype(np.float32) / 255.0\n",
    "        Y_full = np.squeeze(loadmat(str(y_filepath))[f'gt_{half_type}']).astype(np.int64)\n",
    "        \n",
    "        X_tensor = torch.from_numpy(X_full).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        Y_tensor = torch.from_numpy(Y_full).unsqueeze(0).to(device)\n",
    "        return X_tensor, Y_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading raw volume: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def stitch_inference(model, X_full):\n",
    "    \"\"\"\n",
    "    Cuts the 256^3 volume into 8 patches, runs inference, and stitches them back.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    stitched_seg = torch.zeros((1, NUM_CLASSES, FULL_VOLUME_SIZE, FULL_VOLUME_SIZE, FULL_VOLUME_SIZE), device=device)\n",
    "    stitched_recon = torch.zeros((1, 1, FULL_VOLUME_SIZE, FULL_VOLUME_SIZE, FULL_VOLUME_SIZE), device=device)\n",
    "    \n",
    "    print(\"Starting Stitching Loop...\")\n",
    "    for pd, ph, pw in itertools.product(range(SLICES_PER_AXIS), repeat=3):\n",
    "        d_start, d_end = pd * PATCH_SIZE, (pd + 1) * PATCH_SIZE\n",
    "        h_start, h_end = ph * PATCH_SIZE, (ph + 1) * PATCH_SIZE\n",
    "        w_start, w_end = pw * PATCH_SIZE, (pw + 1) * PATCH_SIZE\n",
    "        \n",
    "        X_patch = X_full[:, :, d_start:d_end, h_start:h_end, w_start:w_end]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            seg_patch, recon_patch,_,_ = model(X_patch)\n",
    "            \n",
    "        stitched_seg[:, :, d_start:d_end, h_start:h_end, w_start:w_end] = seg_patch\n",
    "        stitched_recon[:, :, d_start:d_end, h_start:h_end, w_start:w_end] = recon_patch\n",
    "        \n",
    "        print(f\"  Processed Patch: [{d_start}:{d_end}, {h_start}:{h_end}, {w_start}:{w_end}]\")\n",
    "\n",
    "    return stitched_seg, stitched_recon\n",
    "\n",
    "\n",
    "def calculate_metrics(stitched_logits, Y_gt):\n",
    "    \"\"\"Calculates IoU, ME, and AUROC.\"\"\"\n",
    "    print(\"\\n--- Calculating Full Volume Metrics ---\")\n",
    "    probs = torch.softmax(stitched_logits, dim=1)\n",
    "    preds = torch.argmax(stitched_logits, dim=1)\n",
    "    \n",
    "    y_true_flat = Y_gt.cpu().numpy().flatten()\n",
    "    y_pred_flat = preds.cpu().numpy().flatten()\n",
    "    \n",
    "    # Mean Error\n",
    "    me = 1.0 - np.mean(y_true_flat == y_pred_flat)\n",
    "    \n",
    "    # IoU\n",
    "    class_ious = []\n",
    "    class_dice = []\n",
    "    print(f\"\\n[Intersection over Union]\")\n",
    "    for c in range(NUM_CLASSES):\n",
    "        intersection = np.sum((y_true_flat == c) & (y_pred_flat == c))\n",
    "        union = np.sum((y_true_flat == c) | (y_pred_flat == c))\n",
    "        \n",
    "        if union == 0:\n",
    "            iou = 1.0 if intersection == 0 else 0.0\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "            dice = (2*iou)/(1+iou)\n",
    "        print(f\"  Class {c} IoU: {iou:.4f} | Dice {dice:.4f} \")\n",
    "        class_ious.append(iou)\n",
    "        class_dice.append(dice)\n",
    "\n",
    "        \n",
    "    mIoU_all = np.mean(class_ious)\n",
    "    Dice_all = np.mean(class_dice)\n",
    "    \n",
    "    # AUROC\n",
    "    auroc = 0.0\n",
    "    if roc_auc_score is not None:\n",
    "        print(\"\\n[AUROC Calculation...]\")\n",
    "        try:\n",
    "            y_probs = probs.cpu().numpy()[0] # (C, D, H, W)\n",
    "            n_classes = y_probs.shape[0]\n",
    "            # Flatten to (N_voxels, C)\n",
    "            y_probs_flat = np.transpose(y_probs, (1, 2, 3, 0)).reshape(-1, n_classes)\n",
    "            auroc = roc_auc_score(y_true_flat, y_probs_flat, multi_class='ovr', average='macro')\n",
    "        except Exception as e:\n",
    "            print(f\"  AUROC Error: {e}\")\n",
    "            auroc = -1.0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"FINAL METRICS for Column {TEST_COLUMN_ID}\")\n",
    "    print(f\"  Mean Error (ME):       {me:.4f}\")\n",
    "    print(f\"  Mean IoU :             {mIoU_all:.4f}\")\n",
    "    print(f\"  Mean Dice :            {Dice_all:.4f}\")\n",
    "    print(f\"  AUROC (Macro):         {auroc:.4f}\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "\n",
    "def visualize_full_slice(X_tensor, Y_tensor, stitched_seg, stitched_recon, save_path, slice_idx=FULL_VOLUME_SIZE // 2):\n",
    "    \"\"\"Saves a central slice using the CUSTOM COLORMAP.\"\"\"\n",
    "    pred_seg_tensor = torch.argmax(stitched_seg, dim=1) \n",
    "    \n",
    "    x_np = X_tensor[0, 0, slice_idx, :, :].cpu().numpy()\n",
    "    y_np = Y_tensor[0, slice_idx, :, :].cpu().numpy()\n",
    "    recon_np = stitched_recon[0, 0, slice_idx, :, :].cpu().numpy()\n",
    "    pred_np = pred_seg_tensor[0, slice_idx, :, :].cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "    titles = ['Input', 'Ground Truth', 'Reconstruction', 'Prediction']\n",
    "    data = [x_np, y_np, recon_np, pred_np]\n",
    "    \n",
    "    # Use custom map for GT and Pred\n",
    "    custom_cmap = get_custom_colormap()\n",
    "    cmaps = ['gray', custom_cmap, 'gray', custom_cmap]\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        # Vmax=3 for the 4 classes (0,1,2,3)\n",
    "        vmax = NUM_CLASSES - 1 if i == 1 or i == 3 else None\n",
    "        vmin = 0 if i == 1 or i == 3 else None\n",
    "        interp = 'nearest' if i == 1 or i == 3 else None\n",
    "        \n",
    "        im = ax.imshow(data[i], cmap=cmaps[i], interpolation=interp, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(titles[i], fontsize=18)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        if i == 1 or i == 3:\n",
    "             cbar = plt.colorbar(im, ax=ax, ticks=range(NUM_CLASSES), fraction=0.046, pad=0.04)\n",
    "             cbar.ax.set_yticklabels(['Water', 'Oil', 'Solids', 'Gas'])\n",
    "\n",
    "    plt.suptitle(f\"Full Resolution Inference - Test Column {TEST_COLUMN_ID} - Slice {slice_idx}\", fontsize=24)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(f\"Visualization saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738cf7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Model weights not found at /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/Trained_models/VAE_val_finalVAE1.pth\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/d2/4/167803/miniforge3/envs/MBML/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Load Model\n",
    "    #model = MultiTaskNet(in_channels=1, num_classes=NUM_CLASSES, latent_dim=LATENT_DIM).to(device)\n",
    "    model = VAE(in_channels=1, latent_dim=LATENT_DIM, NUM_CLASSES=NUM_CLASSES)\n",
    "\n",
    "    if MODEL_PATH.exists():\n",
    "        print(f\"Loading weights from: {MODEL_PATH}\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    else:\n",
    "        print(f\"❌ Model weights not found at {MODEL_PATH}\")\n",
    "        sys.exit(1)\n",
    "    for i in TEST_COLUMN_ID: \n",
    "        TEST_COLUMN_ID = i\n",
    "        # 2. Load Data (Column 35, Top Half)\n",
    "        X_full_tensor, Y_full_tensor = load_raw_volume(column_num=TEST_COLUMN_ID, half_type='top')\n",
    "    \n",
    "        # 3. Stitching Inference\n",
    "        stitched_seg, stitched_recon = stitch_inference(model, X_full_tensor)\n",
    "\n",
    "        # 4. Metrics Calculation\n",
    "        calculate_metrics(stitched_seg, Y_full_tensor)\n",
    "\n",
    "        # 5. Visualization\n",
    "        save_full_path = Path.cwd() / f\"full_vae_best_col{i}.png\"\n",
    "        visualize_full_slice(X_full_tensor, Y_full_tensor, stitched_seg, stitched_recon, save_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/Trained_models/VAE_val_finalVAE1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1151451/1574469701.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /dtu/blackhole/1b/167803/deep_learning_214776/extracted_datasets/datasets_processed_latest/Column_1/B/top.mat\n",
      "Starting Stitching Loop...\n",
      "  Processed Patch: [0:128, 0:128, 0:128]\n",
      "  Processed Patch: [0:128, 0:128, 128:256]\n",
      "  Processed Patch: [0:128, 128:256, 0:128]\n",
      "  Processed Patch: [0:128, 128:256, 128:256]\n",
      "  Processed Patch: [128:256, 0:128, 0:128]\n",
      "  Processed Patch: [128:256, 0:128, 128:256]\n",
      "  Processed Patch: [128:256, 128:256, 0:128]\n",
      "  Processed Patch: [128:256, 128:256, 128:256]\n",
      "\n",
      "--- Calculating Full Volume Metrics ---\n",
      "\n",
      "[Intersection over Union]\n",
      "  Class 0 IoU: 0.8816 | Dice 0.9371 \n",
      "  Class 1 IoU: 0.9200 | Dice 0.9583 \n",
      "  Class 2 IoU: 0.9403 | Dice 0.9692 \n",
      "  Class 3 IoU: 0.0018 | Dice 0.0036 \n",
      "\n",
      "[AUROC Calculation...]\n",
      "\n",
      "==============================\n",
      "FINAL METRICS for Column 1\n",
      "  Mean Error (ME):       0.0458\n",
      "  Mean IoU :             0.6859\n",
      "  Mean Dice :            0.7171\n",
      "  AUROC (Macro):         0.9583\n",
      "==============================\n",
      "\n",
      "✅ Visualization saved to: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/scripts/full_vae_best_col1.png\n",
      "Loading: /dtu/blackhole/1b/167803/deep_learning_214776/extracted_datasets/datasets_processed_latest/Column_2/B/top.mat\n",
      "Starting Stitching Loop...\n",
      "  Processed Patch: [0:128, 0:128, 0:128]\n",
      "  Processed Patch: [0:128, 0:128, 128:256]\n",
      "  Processed Patch: [0:128, 128:256, 0:128]\n",
      "  Processed Patch: [0:128, 128:256, 128:256]\n",
      "  Processed Patch: [128:256, 0:128, 0:128]\n",
      "  Processed Patch: [128:256, 0:128, 128:256]\n",
      "  Processed Patch: [128:256, 128:256, 0:128]\n",
      "  Processed Patch: [128:256, 128:256, 128:256]\n",
      "\n",
      "--- Calculating Full Volume Metrics ---\n",
      "\n",
      "[Intersection over Union]\n",
      "  Class 0 IoU: 0.7990 | Dice 0.8883 \n",
      "  Class 1 IoU: 0.7592 | Dice 0.8631 \n",
      "  Class 2 IoU: 0.9106 | Dice 0.9532 \n",
      "  Class 3 IoU: 0.6271 | Dice 0.7709 \n",
      "\n",
      "[AUROC Calculation...]\n",
      "\n",
      "==============================\n",
      "FINAL METRICS for Column 2\n",
      "  Mean Error (ME):       0.0761\n",
      "  Mean IoU :             0.7740\n",
      "  Mean Dice :            0.8689\n",
      "  AUROC (Macro):         0.9778\n",
      "==============================\n",
      "\n",
      "✅ Visualization saved to: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/scripts/full_vae_best_col2.png\n",
      "Loading: /dtu/blackhole/1b/167803/deep_learning_214776/extracted_datasets/datasets_processed_latest/Column_37/B/top.mat\n",
      "Starting Stitching Loop...\n",
      "  Processed Patch: [0:128, 0:128, 0:128]\n",
      "  Processed Patch: [0:128, 0:128, 128:256]\n",
      "  Processed Patch: [0:128, 128:256, 0:128]\n",
      "  Processed Patch: [0:128, 128:256, 128:256]\n",
      "  Processed Patch: [128:256, 0:128, 0:128]\n",
      "  Processed Patch: [128:256, 0:128, 128:256]\n",
      "  Processed Patch: [128:256, 128:256, 0:128]\n",
      "  Processed Patch: [128:256, 128:256, 128:256]\n",
      "\n",
      "--- Calculating Full Volume Metrics ---\n",
      "\n",
      "[Intersection over Union]\n",
      "  Class 0 IoU: 0.7580 | Dice 0.8624 \n",
      "  Class 1 IoU: 0.9190 | Dice 0.9578 \n",
      "  Class 2 IoU: 0.8416 | Dice 0.9140 \n",
      "  Class 3 IoU: 0.5280 | Dice 0.6911 \n",
      "\n",
      "[AUROC Calculation...]\n",
      "\n",
      "==============================\n",
      "FINAL METRICS for Column 37\n",
      "  Mean Error (ME):       0.1204\n",
      "  Mean IoU :             0.7617\n",
      "  Mean Dice :            0.8563\n",
      "  AUROC (Macro):         0.9729\n",
      "==============================\n",
      "\n",
      "✅ Visualization saved to: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/scripts/full_vae_best_col37.png\n",
      "Loading: /dtu/blackhole/1b/167803/deep_learning_214776/extracted_datasets/datasets_processed_latest/Column_38/B/top.mat\n",
      "Starting Stitching Loop...\n",
      "  Processed Patch: [0:128, 0:128, 0:128]\n",
      "  Processed Patch: [0:128, 0:128, 128:256]\n",
      "  Processed Patch: [0:128, 128:256, 0:128]\n",
      "  Processed Patch: [0:128, 128:256, 128:256]\n",
      "  Processed Patch: [128:256, 0:128, 0:128]\n",
      "  Processed Patch: [128:256, 0:128, 128:256]\n",
      "  Processed Patch: [128:256, 128:256, 0:128]\n",
      "  Processed Patch: [128:256, 128:256, 128:256]\n",
      "\n",
      "--- Calculating Full Volume Metrics ---\n",
      "\n",
      "[Intersection over Union]\n",
      "  Class 0 IoU: 0.8591 | Dice 0.9242 \n",
      "  Class 1 IoU: 0.9103 | Dice 0.9531 \n",
      "  Class 2 IoU: 0.9016 | Dice 0.9482 \n",
      "  Class 3 IoU: 0.7704 | Dice 0.8703 \n",
      "\n",
      "[AUROC Calculation...]\n",
      "\n",
      "==============================\n",
      "FINAL METRICS for Column 38\n",
      "  Mean Error (ME):       0.0664\n",
      "  Mean IoU :             0.8603\n",
      "  Mean Dice :            0.9240\n",
      "  AUROC (Macro):         0.9918\n",
      "==============================\n",
      "\n",
      "✅ Visualization saved to: /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/scripts/full_vae_best_col38.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Load Model\n",
    "    #model = MultiTaskNet(in_channels=1, num_classes=NUM_CLASSES, latent_dim=LATENT_DIM).to(device)\n",
    "    model = VAE(in_channels=1, latent_dim=LATENT_DIM, NUM_CLASSES=NUM_CLASSES)\n",
    "\n",
    "    if MODEL_PATH.exists():\n",
    "        print(f\"Loading weights from: {MODEL_PATH}\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    else:\n",
    "        print(f\"❌ Model weights not found at {MODEL_PATH}\")\n",
    "        sys.exit(1)\n",
    "    for i in TEST_COLUMN_ID: \n",
    "        TEST_COLUMN_ID = i\n",
    "        # 2. Load Data (Column 35, Top Half)\n",
    "        X_full_tensor, Y_full_tensor = load_raw_volume(column_num=TEST_COLUMN_ID, half_type='top')\n",
    "    \n",
    "        # 3. Stitching Inference\n",
    "        stitched_seg, stitched_recon = stitch_inference(model, X_full_tensor)\n",
    "\n",
    "        # 4. Metrics Calculation\n",
    "        calculate_metrics(stitched_seg, Y_full_tensor)\n",
    "\n",
    "        # 5. Visualization\n",
    "        save_full_path = Path.cwd() / f\"full_vae_final_col{i}.png\"\n",
    "        visualize_full_slice(X_full_tensor, Y_full_tensor, stitched_seg, stitched_recon, save_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Visualization saved to: vae_0.png\n",
      "✅ Visualization saved to: vae_1.png\n",
      "✅ Visualization saved to: vae_2.png\n",
      "✅ Visualization saved to: vae_3.png\n",
      "✅ Visualization saved to: vae_4.png\n",
      "✅ Visualization saved to: vae_5.png\n",
      "✅ Visualization saved to: vae_6.png\n",
      "✅ Visualization saved to: vae_7.png\n",
      "✅ Visualization saved to: vae_8.png\n",
      "✅ Visualization saved to: vae_9.png\n",
      "✅ Visualization saved to: vae_10.png\n",
      "✅ Visualization saved to: vae_11.png\n",
      "✅ Visualization saved to: vae_12.png\n",
      "✅ Visualization saved to: vae_13.png\n",
      "✅ Visualization saved to: vae_14.png\n",
      "✅ Visualization saved to: vae_15.png\n",
      "✅ Visualization saved to: vae_16.png\n",
      "✅ Visualization saved to: vae_17.png\n",
      "✅ Visualization saved to: vae_18.png\n",
      "✅ Visualization saved to: vae_19.png\n",
      "✅ Visualization saved to: vae_20.png\n",
      "✅ Visualization saved to: vae_21.png\n",
      "✅ Visualization saved to: vae_22.png\n",
      "✅ Visualization saved to: vae_23.png\n",
      "✅ Visualization saved to: vae_24.png\n",
      "✅ Visualization saved to: vae_25.png\n",
      "✅ Visualization saved to: vae_26.png\n",
      "✅ Visualization saved to: vae_27.png\n",
      "✅ Visualization saved to: vae_28.png\n",
      "✅ Visualization saved to: vae_29.png\n",
      "✅ Visualization saved to: vae_30.png\n",
      "✅ Visualization saved to: vae_31.png\n",
      "✅ Visualization saved to: vae_32.png\n",
      "✅ Visualization saved to: vae_33.png\n",
      "✅ Visualization saved to: vae_34.png\n",
      "✅ Visualization saved to: vae_35.png\n",
      "✅ Visualization saved to: vae_36.png\n",
      "✅ Visualization saved to: vae_37.png\n",
      "✅ Visualization saved to: vae_38.png\n",
      "✅ Visualization saved to: vae_39.png\n",
      "✅ Visualization saved to: vae_40.png\n",
      "✅ Visualization saved to: vae_41.png\n",
      "✅ Visualization saved to: vae_42.png\n",
      "✅ Visualization saved to: vae_43.png\n",
      "✅ Visualization saved to: vae_44.png\n",
      "✅ Visualization saved to: vae_45.png\n",
      "✅ Visualization saved to: vae_46.png\n",
      "✅ Visualization saved to: vae_47.png\n",
      "✅ Visualization saved to: vae_48.png\n",
      "✅ Visualization saved to: vae_49.png\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    img_save = f\"vae_{i}.png\"\n",
    "    visualize_full_slice(X_full_tensor, Y_full_tensor, stitched_seg, stitched_recon, img_save, slice_idx=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MBML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
