{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9311186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from IPython.display import Image, display, clear_output\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import Tensor\n",
    "from torch.distributions import Normal\n",
    "from torchvision.utils import make_grid\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320a575a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 28, 28]) torch.Size([1, 512, 64, 64])\n",
      "Unet Output shape: [1, 2, 388, 388]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def double_conv_unpadded(in_channels, out_channels):\n",
    "    \"\"\"\n",
    "    Two consecutive 3x3 unpadded convolutions (padding=0).\n",
    "    Total size reduction per block is 4 pixels (2 from first conv + 2 from second conv).\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net Encoder step: Conv-Conv block followed by Max Pooling.\n",
    "    Returns both the feature map (for skip) and the pooled map (for next block).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Conv block\n",
    "        self.conv = double_conv_unpadded(in_channels, out_channels)\n",
    "        # Pooling layer \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_output = self.conv(x)\n",
    "        pool_output = self.pool(conv_output)\n",
    "        return conv_output, pool_output\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net Decoder step: Up-convolution, Cropping, Concatenation, and Refinement.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.up = nn.ConvTranspose2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=2, \n",
    "            stride=2\n",
    "        )\n",
    "        self.conv = double_conv_unpadded(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_bottom, x_skip):\n",
    "        \n",
    "        # Double the size of the feature map from the layer below\n",
    "        x_up = self.up(x_bottom) \n",
    "        \n",
    "        # The skip connection (x_skip) is LARGER than the upsampled map (x_up) in this architecture.\n",
    "        # We must calculate the difference and crop the larger x_skip to match the smaller x_up.\n",
    "        diff_h = x_skip.size(2) - x_up.size(2)\n",
    "        diff_w = x_skip.size(3) - x_up.size(3)\n",
    "        \n",
    "        # ensure the sizes match\n",
    "        if diff_h < 0 or diff_w < 0:\n",
    "             raise RuntimeError(f\"Cropping error: x_up ({x_up.shape}) is unexpectedly larger than x_skip ({x_skip.shape}). Check encoder padding.\")\n",
    "\n",
    "        # apply croping to the larger x_skip tensor to match x_up size\n",
    "        x_skip_cropped = x_skip[:, :, \n",
    "            diff_h // 2 : x_skip.size(2) - diff_h + diff_h // 2, \n",
    "            diff_w // 2 : x_skip.size(3) - diff_w + diff_w // 2\n",
    "        ]\n",
    "        \n",
    "        # concatenate the x_up with skipped conection\n",
    "        x_combined = torch.cat([x_up, x_skip_cropped], dim=1)\n",
    "        \n",
    "        # Pass combined map through the convolutional block\n",
    "        return self.conv(x_combined)\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Block-based U-Net Encoder: 5 levels of downsampling.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # blocks \n",
    "        self.block1 = EncoderBlock(1, 64)        # 1 -> 64 channels\n",
    "        self.block2 = EncoderBlock(64, 128)      # 64 -> 128 channels\n",
    "        self.block3 = EncoderBlock(128, 256)     # 128 -> 256 channels\n",
    "        self.block4 = EncoderBlock(256, 512)     # 256 -> 512 channels\n",
    "        \n",
    "        # bttleneck \n",
    "        self.bottleneck = double_conv_unpadded(512, 1024) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, p1 = self.block1(x)\n",
    "        x2, p2 = self.block2(p1)\n",
    "        x3, p3 = self.block3(p2)\n",
    "        x4, p4 = self.block4(p3)\n",
    "        x5 = self.bottleneck(p4)\n",
    "        \n",
    "        return x1, x2, x3, x4, x5\n",
    "    \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_classes=1): \n",
    "        super().__init__()\n",
    "        \n",
    "        # 1024 -> 512 (combines with Encoder x4, 512 channels)\n",
    "        self.upconv4 = DecoderBlock(in_channels=1024, skip_channels=512, out_channels=512) \n",
    "        \n",
    "        # 512 -> 256 (Combines with Encoder x3, 256 channels)\n",
    "        self.upconv3 = DecoderBlock(in_channels=512, skip_channels=256, out_channels=256) \n",
    "        \n",
    "        # 256 -> 128 (Combines with Encoder x2, 128 channels)\n",
    "        self.upconv2 = DecoderBlock(in_channels=256, skip_channels=128, out_channels=128) \n",
    "        \n",
    "        # 128 -> 64 (Combines with Encoder x1, 64 channels)\n",
    "        self.upconv1 = DecoderBlock(in_channels=128, skip_channels=64, out_channels=64) \n",
    "        \n",
    "        self.out_conv = nn.Conv2d(64, num_classes, kernel_size=1) \n",
    "\n",
    "    def forward(self, x5_bottleneck, x4_skip, x3_skip, x2_skip, x1_skip):\n",
    "        d4 = self.upconv4(x5_bottleneck, x4_skip)\n",
    "        d3 = self.upconv3(d4, x3_skip)\n",
    "        d2 = self.upconv2(d3, x2_skip)\n",
    "        d1 = self.upconv1(d2, x1_skip)\n",
    "        \n",
    "        return self.out_conv(d1)\n",
    "\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1, x2, x3, x4, x5 = self.encoder(x)\n",
    "        print(x5.shape, x4.shape)\n",
    "        \n",
    "        # Decoder\n",
    "        output = self.decoder(x5, x4, x3, x2, x1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Test with the input size from your diagram (572x572)\n",
    "    model = Unet(num_classes=2) \n",
    "    \n",
    "    # Input is 1 sample, 1 channel, 572x572\n",
    "    test_input = torch.randn(1, 1, 572, 572) \n",
    "    \n",
    "    # Run the model\n",
    "    output = model(test_input)\n",
    "    \n",
    "    # Expected output size (original size - (4 pixels * 4 downsampling steps) = 572 - 16 = 556? No, cropping handles this)\n",
    "    # The output size should match the size after the last unpadded double conv:\n",
    "    # 572 -> 568 (x1)\n",
    "    # The decoder will align the final output to this size.\n",
    "    print(f\"Unet Output shape: {list(output.shape)}\") # Expected: [1, 2, 568, 568]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train_loader, test_laoder, and valid_loader.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cbef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(lr=lr)\n",
    "criterion_l = nn.MSELoss()\n",
    "criterion_u = nn.CrossEntropyLoss\n",
    "valid_loss = []\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    batch_loss = []\n",
    "    for (x_l, y_l), (x_u, _) in zip(cycle(train_loader_l), train_loader_u):\n",
    "        x = input # some kind of data splittin here \n",
    "        output = Unet(x)\n",
    "\n",
    "        loss_l = criterion(output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss.append(loss)\n",
    "        \n",
    "        loss_\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MBML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
