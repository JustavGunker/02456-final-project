Loading CUDA module...
Loading Conda...
Environment 'MBML' activated.
Starting Python script...
Using device: cuda
Using device: cuda
Found 28 image/label pairs.
--- success ---
Found 119 unlabeled images.
--- success ---
Pre-training Autoencoder

--- Pre-train Epoch 1/150 ---
Batch 0 | Recon Loss: 0.2302
Batch 35 | Recon Loss: 0.0771
Batch 70 | Recon Loss: 0.0579
Avg Epoch loss: 7.8797

--- Pre-train Epoch 2/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 3/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 4/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 5/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 6/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 7/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 8/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 9/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 10/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 11/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 12/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 13/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 14/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 15/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 16/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 17/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 18/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 19/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 20/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 21/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 22/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 23/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 24/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 25/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 26/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 27/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 28/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 29/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 30/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 31/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 32/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 33/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 34/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 35/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 36/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 37/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 38/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 39/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 40/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 41/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 42/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 43/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 44/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 45/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 46/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 47/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 48/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 49/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 50/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 51/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 52/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 53/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 54/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 55/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 56/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 57/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 58/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 59/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 60/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 61/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 62/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 63/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 64/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 65/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 66/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 67/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 68/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 69/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 70/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 71/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 72/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 73/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 74/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 75/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 76/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 77/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 78/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 79/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 80/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 81/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 82/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 83/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 84/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 85/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 86/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 87/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 88/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 89/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 90/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 91/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 92/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 93/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 94/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 95/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 96/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 97/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 98/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 99/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 100/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 101/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 102/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 103/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 104/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 105/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 106/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 107/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 108/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 109/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 110/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 111/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 112/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 113/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 114/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 115/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 116/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 117/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 118/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 119/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 120/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 121/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 122/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 123/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 124/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 125/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 126/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 127/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 128/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 129/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 130/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 131/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 132/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 133/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 134/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 135/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 136/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 137/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 138/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 139/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 140/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 141/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 142/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 143/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 144/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 145/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 146/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 147/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 148/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 149/150 ---
Avg Epoch loss: 0.0000

--- Pre-train Epoch 150/150 ---
Avg Epoch loss: 0.0000
--- Pre-training Finished ---
Saving encoder weights to /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/Trained_models/pretrained_ae_encoder.pth
Job Finished.

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27046381: <pretain_AE_job> in cluster <dcc> Done

Job <pretain_AE_job> was submitted from host <gbarlogin1> by user <s214776> in cluster <dcc> at Mon Nov 17 02:41:33 2025
Job was executed on host(s) <4*n-62-13-16>, in queue <gpul40s>, as user <s214776> in cluster <dcc> at Mon Nov 17 02:41:35 2025
</zhome/d2/4/167803> was used as the home directory.
</zhome/d2/4/167803/Desktop/Deep_project/02456-final-project> was used as the working directory.
Started at Mon Nov 17 02:41:35 2025
Terminated at Mon Nov 17 02:46:33 2025
Results reported at Mon Nov 17 02:46:33 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
### --- Job Name ---
#BSUB -J pretain_AE_job

### --- Log files ---
#BSUB -o /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/logs/pretain_AE_job_%J.out
#BSUB -e /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/logs/pretain_AE_job_%J.err
### --- Resource Requests ---
#BSUB -q gpul40s            # Request GPU queue
#BSUB -gpu "num=1:mode=exclusive_process" # Request 1 GPU, all to myself
#BSUB -n 4                    # Request 4 CPU cores
#BSUB -R "rusage[mem=16GB]"   # Request 16GB memory
#BSUB -W 04:00                # 4 hour runtime limit

### --- Setup Environment ---
echo "Loading CUDA module..."
module load cuda/11.6         # Load CUDA module

echo "Loading Conda..."
# This is the full, correct path to your environment
source /zhome/d2/4/167803/miniforge3/bin/activate MBML
echo "Environment 'MBML' activated."

### --- Run Script ---
echo "Starting Python script..."
python /zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/Model/pretrain_AE.py

echo "Job Finished."
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   846.00 sec.
    Max Memory :                                 1330 MB
    Average Memory :                             1259.80 MB
    Total Requested Memory :                     65536.00 MB
    Delta Memory :                               64206.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   340 sec.
    Turnaround time :                            300 sec.

The output (if any) is above this job summary.



PS:

Read file </zhome/d2/4/167803/Desktop/Deep_project/02456-final-project/logs/pretain_AE_job_27046381.err> for stderr output of this job.

