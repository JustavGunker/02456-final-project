{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9311186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "from torchvision.transforms.functional import pad, center_crop\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320a575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def double_conv_unpadded(in_channels, out_channels):\n",
    "    \"\"\"\n",
    "    Two consecutive 3x3 unpadded convolutions (padding=0).\n",
    "    Total size reduction per block is 4 pixels (2 from first conv + 2 from second conv).\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net Encoder step: Conv-Conv block followed by Max Pooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = double_conv_unpadded(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_output = self.conv(x)\n",
    "        pool_output = self.pool(conv_output)\n",
    "        return conv_output, pool_output\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net Decoder step: Up-convolution, Cropping, Concatenation, and Refinement.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.up = nn.ConvTranspose2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=2, \n",
    "            stride=2\n",
    "        )\n",
    "        self.conv = double_conv_unpadded(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_bottom, x_skip):\n",
    "        \n",
    "        x_up = self.up(x_bottom) \n",
    "        \n",
    "        diff_h = x_skip.size(2) - x_up.size(2)\n",
    "        diff_w = x_skip.size(3) - x_up.size(3)\n",
    "        \n",
    "        if diff_h < 0 or diff_w < 0:\n",
    "             raise RuntimeError(f\"Cropping error: x_up ({x_up.shape}) is larger than x_skip ({x_skip.shape}).\")\n",
    "\n",
    "        # Corrected cropping logic for odd/even mismatches\n",
    "        start_h = diff_h // 2\n",
    "        end_h = x_skip.size(2) - (diff_h - start_h)\n",
    "        start_w = diff_w // 2\n",
    "        end_w = x_skip.size(3) - (diff_w - start_w)\n",
    "\n",
    "        x_skip_cropped = x_skip[:, :, start_h:end_h, start_w:end_w]\n",
    "        \n",
    "        x_combined = torch.cat([x_up, x_skip_cropped], dim=1)\n",
    "        \n",
    "        return self.conv(x_combined)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Block-based U-Net Encoder: 5 levels of downsampling.\n",
    "    The bottleneck is the final convolution block *within* the encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = EncoderBlock(1, 64)        # 1 -> 64 channels\n",
    "        self.block2 = EncoderBlock(64, 128)      # 64 -> 128 channels\n",
    "        self.block3 = EncoderBlock(128, 256)     # 128 -> 256 channels\n",
    "        self.block4 = EncoderBlock(256, 512)     # 256 -> 512 channels\n",
    "        \n",
    "        # This is the U-Net bottleneck\n",
    "        self.bottleneck = double_conv_unpadded(512, 1024) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, p1 = self.block1(x)\n",
    "        x2, p2 = self.block2(p1)\n",
    "        x3, p3 = self.block3(p2)\n",
    "        x4, p4 = self.block4(p3)\n",
    "        x5 = self.bottleneck(p4) # x5 is the bottleneck tensor\n",
    "        \n",
    "        return x1, x2, x3, x4, x5\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_classes=1): \n",
    "        super().__init__()\n",
    "        # bottle be 1024 channels\n",
    "        self.upconv4 = DecoderBlock(in_channels=1024, skip_channels=512, out_channels=512) \n",
    "        self.upconv3 = DecoderBlock(in_channels=512, skip_channels=256, out_channels=256) \n",
    "        self.upconv2 = DecoderBlock(in_channels=256, skip_channels=128, out_channels=128) \n",
    "        self.upconv1 = DecoderBlock(in_channels=128, skip_channels=64, out_channels=64) \n",
    "        self.out_conv = nn.Conv2d(64, num_classes, kernel_size=1) \n",
    "\n",
    "    def forward(self, x5_bottleneck, x4_skip, x3_skip, x2_skip, x1_skip):\n",
    "        d4 = self.upconv4(x5_bottleneck, x4_skip)\n",
    "        d3 = self.upconv3(d4, x3_skip)\n",
    "        d2 = self.upconv2(d3, x2_skip)\n",
    "        d1 = self.upconv1(d2, x1_skip)\n",
    "        \n",
    "        return self.out_conv(d1)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5 = self.encoder(x)\n",
    "        \n",
    "        # Decoder\n",
    "        output = self.decoder(x5, x4, x3, x2, x1)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cbef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading data from: /Users/holgermaxfloelyng/Desktop/BioMed/MSc_Biomed/SEM_2/02456_Deep_learning/Project/archive/images\n",
      "Loading labels from: /Users/holgermaxfloelyng/Desktop/BioMed/MSc_Biomed/SEM_2/02456_Deep_learning/Project/archive/masks\n",
      "Dataset created with 3064 image/mask pairs.\n",
      "--- DEBUG MODE: Using a subset of 50 images ---\n",
      "Splitting 50 images into: Train (35), Val (7), Test (8)\n",
      "DataLoaders created:\n",
      "  Train: 35 images\n",
      "  Validation: 7 images\n",
      "  Test: 8 images\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([5, 1, 129, 129])) must be the same as input size (torch.Size([5, 1, 4, 4]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 248\u001b[0m\n\u001b[1;32m    245\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# start train\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# visualisse\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_loader) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[3], line 187\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m    185\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m--> 187\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    189\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/MBML/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/MBML/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/MBML/lib/python3.10/site-packages/torch/nn/modules/loss.py:821\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/MBML/lib/python3.10/site-packages/torch/nn/functional.py:3639\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3636\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3641\u001b[0m     )\n\u001b[1;32m   3643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m   3644\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[1;32m   3645\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([5, 1, 129, 129])) must be the same as input size (torch.Size([5, 1, 4, 4]))"
     ]
    }
   ],
   "source": [
    "class BrainScanDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Made by AI\n",
    "    Loads 2D .png images and .png masks from 'images' and 'masks' folders.\n",
    "    Resizes images to 572x572 (U-Net input) and\n",
    "    masks to 388x388 (U-Net output).\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, mask_dir, \n",
    "                 input_size=(572, 572), \n",
    "                 output_size=(388, 388)):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Find all image files\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
    "        \n",
    "        # Create corresponding mask paths\n",
    "        self.mask_paths = []\n",
    "        for img_path in self.image_paths:\n",
    "            filename = os.path.basename(img_path)\n",
    "            mask_path = os.path.join(mask_dir, filename)\n",
    "            \n",
    "            if os.path.exists(mask_path):\n",
    "                self.mask_paths.append(mask_path)\n",
    "            else:\n",
    "                print(f\"Warning: Missing mask for image {img_path}\")\n",
    "\n",
    "        if not self.image_paths:\n",
    "            print(f\"Warning: No '*.png' files found in {image_dir}\")\n",
    "        if not self.mask_paths:\n",
    "            print(f\"Warning: No corresponding '*.png' files found in {mask_dir}\")\n",
    "\n",
    "        print(f\"Dataset created with {len(self.image_paths)} image/mask pairs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert(\"L\") # Load as grayscale (1 channel)\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Load as grayscale (1 channel)\n",
    "\n",
    "        # --- Apply resizing ---\n",
    "        # 1. Resize image to 572x572 (U-Net Input)\n",
    "        img_resized = TF.resize(image, self.input_size, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "        \n",
    "        # 2. Resize mask to 388x388 (U-Net Output)\n",
    "        mask_resized = TF.resize(mask, self.output_size, interpolation=TF.InterpolationMode.NEAREST)\n",
    "\n",
    "        # --- Convert to Tensors ---\n",
    "        img_tensor = TF.to_tensor(img_resized)\n",
    "        mask_tensor = TF.to_tensor(mask_resized)\n",
    "\n",
    "        # Binarize mask (image is 0 or 255)\n",
    "        mask_tensor = (mask_tensor > 0.5).float() # > 0.5 to be safe\n",
    "        \n",
    "        # Image is already normalized [0, 1] by to_tensor()\n",
    "        \n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "\n",
    "def get_dataloaders(image_dir, mask_dir, batch_size=4, debug_subset_size=None):\n",
    "    \"\"\"\n",
    "    Made with AI\n",
    "    Creates and splits the dataset into train, validation, and test loaders.\n",
    "    \"\"\"\n",
    "    # Create the training/validation dataset from the TrainingData folder\n",
    "    full_dataset = BrainScanDataset(image_dir=image_dir, mask_dir=mask_dir)\n",
    "    \n",
    "    total_size = len(full_dataset)\n",
    "    if total_size == 0:\n",
    "        raise ValueError(\"Dataset is empty. Check file paths and file types.\")\n",
    "    \n",
    "    if debug_subset_size is not None:\n",
    "        if debug_subset_size > total_size:\n",
    "            print(f\"Warning: debug_subset_size ({debug_subset_size}) is larger than total dataset ({total_size}). Using total dataset.\")\n",
    "        else:\n",
    "            print(f\"--- DEBUG MODE: Using a subset of {debug_subset_size} images ---\")\n",
    "            # Split the full dataset into a tiny subset and the rest\n",
    "            debug_set, _ = random_split(full_dataset, [debug_subset_size, total_size - debug_subset_size])\n",
    "            full_dataset = debug_set \n",
    "            total_size = len(full_dataset)\n",
    "    \n",
    "    # Split the training data into train and validation sets\n",
    "    train_size = int(total_size * 0.7) # 70% for train\n",
    "    val_size = int(total_size * 0.15)  # 15% for val\n",
    "    test_size = total_size - train_size - val_size # 15% for test\n",
    "    \n",
    "    print(f\"Splitting {total_size} images into: Train ({train_size}), Val ({val_size}), Test ({test_size})\")\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders with num_workers for speed\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"DataLoaders created:\")\n",
    "    print(f\"  Train: {len(train_loader.dataset)} images\")\n",
    "    print(f\"  Validation: {len(val_loader.dataset)} images\")\n",
    "    print(f\"  Test: {len(test_loader.dataset)} images\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def visualize_results(model, loader, device, num_to_show=3):\n",
    "    \"\"\"\n",
    "    Made by AI\n",
    "    Shows a 1x3 plot of (Input, True Mask, Predicted Mask)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Visualizing {num_to_show} validation examples ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        images, masks = next(iter(loader))\n",
    "    except StopIteration:\n",
    "        print(\"Data loader is empty, skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        # Apply sigmoid (since we use BCEWithLogitsLoss) and threshold\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "    images = images.cpu()\n",
    "    masks = masks.cpu()\n",
    "    preds = preds.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(num_to_show, 3, figsize=(15, 5 * num_to_show))\n",
    "    fig.suptitle(\"Model Input vs. True Mask vs. Predicted Mask\", fontsize=16)\n",
    "\n",
    "    for i in range(num_to_show):\n",
    "        if i >= len(images): \n",
    "            break\n",
    "            \n",
    "        ax = axes[i, 0] if num_to_show > 1 else axes[0]\n",
    "        ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(f\"Input Image (Resized {images[i].shape[-2:]})\")\n",
    "        ax.axis('off')\n",
    "\n",
    "        ax = axes[i, 1] if num_to_show > 1 else axes[1]\n",
    "        ax.imshow(masks[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(f\"True Mask (Resized {masks[i].shape[-2:]})\")\n",
    "        ax.axis('off')\n",
    "\n",
    "        ax = axes[i, 2] if num_to_show > 1 else axes[2]\n",
    "        ax.imshow(preds[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(f\"Predicted Mask ({preds[i].shape[-2:]})\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_visualization.png\")\n",
    "    print(\"Visualization saved as 'model_visualization.png'\")\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=5):\n",
    "    \"\"\"\n",
    "    A complete training and validation loop.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        if len(train_loader) == 0:\n",
    "            print(\"Warning: Training data loader is empty.\")\n",
    "            continue\n",
    "        \n",
    "        num_batches = len(train_loader) \n",
    "            \n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            if batch_idx % 2 == 0:\n",
    "                print(f\"Batch {batch_idx}/{len(train_loader)} | Total Loss: {train_loss:.4f}\")\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        if len(val_loader) == 0:\n",
    "            print(\"Warning: Validation data loader is empty.\")\n",
    "            avg_train_loss = train_loss / num_batches\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} Summary | Train Loss: {avg_train_loss:.4f} | Val Loss: N/A\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_train_loss = train_loss / num_batches\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} summary | Avg Train Loss: {avg_train_loss:.4f} | Avg Val Loss: {avg_val_loss:.4f}\\n\")\n",
    "    \n",
    "    print(\"--- Training Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # model \n",
    "    model = Unet(num_classes=1).to(device)\n",
    "        \n",
    "    base_dir = Path.cwd() \n",
    "    DATA_ROOT = base_dir.parent / \"archive\"\n",
    "        \n",
    "    image_folder = DATA_ROOT / \"images\"\n",
    "    mask_folder = DATA_ROOT / \"masks\"\n",
    "        \n",
    "    print(f\"Loading data from: {image_folder}\")\n",
    "    print(f\"Loading labels from: {mask_folder}\")\n",
    "        \n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        image_dir=image_folder,\n",
    "        mask_dir=mask_folder,\n",
    "        batch_size=5,\n",
    "        debug_subset_size= 50)\n",
    "        \n",
    "    # Loss and Optimizer.\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        \n",
    "    # start train\n",
    "    train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=5)\n",
    "        \n",
    "    # visualisse\n",
    "    if len(val_loader) > 0:\n",
    "        visualize_results(model, val_loader, device, num_to_show=3)\n",
    "        \n",
    "    # 6. run test\n",
    "    if len(test_loader) > 0:\n",
    "        print(\"\\n--- Evaluating on Test Set ---\")\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in test_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                test_loss += loss.item()\n",
    "                    \n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo test set to evaluate.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Unet().to(device)\n",
    "output = unet(torch.randn(1,3,256,256).to(device))\n",
    "print(output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MBML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
