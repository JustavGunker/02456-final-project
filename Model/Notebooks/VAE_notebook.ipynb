{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244bcf5d",
   "metadata": {},
   "source": [
    "## VAE notebook\n",
    "This notebook goes through training steps of the 3D VAE model. It may not be possible to run the model in the notebook. Therefore, we refer to VAE_vali.py and ./bash/VAE_run.sh, to test the model yourself.\n",
    "\n",
    "\n",
    "### Important libs and custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731dbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler \n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent.parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from func.loss import KLAnnealing, ComboLoss, FocalLoss, TverskyLoss, kld_loss\n",
    "from func.Models import VAE\n",
    "from func.dataloaders import VolumetricPatchDataset \n",
    "from func.utill import save_predictions, plot_learning_curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379dddb",
   "metadata": {},
   "source": [
    "### Defining hyperparameters. \n",
    "We also switch to gpu using cuda and save important paths. Splitting data into ~90% train, ~5% validation, ~5% test. Initiation of csv file where we save losses and mIoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 1.0, 1.0, 3.0]).to(DEVICE) \n",
    "print(f\"Using Class Weights: {CLASS_WEIGHTS}\")\n",
    "\n",
    "LATENT_DIM = 512\n",
    "NUM_EPOCHS = 400\n",
    "BATCH_SIZE = 3 \n",
    "INPUT_SHAPE = (128, 128, 128) \n",
    "NUM_CLASSES = 4 # Background + 3 segments\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Weights\n",
    "SEG_WEIGHT = 100.0 \n",
    "RECON_WEIGHT = 1.0\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output_VAE_vali\"\n",
    "CSV_PATH =  PROJECT_ROOT / \"stats\" / \"training_log_vae_final.csv\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_PATH = PROJECT_ROOT / \"Trained_models\" / \"VAE_val_best.pth\"\n",
    "SAVE_PATH_FINAL = PROJECT_ROOT / \"Trained_models\" / \"VAE_val_final.pth\"\n",
    "SAVE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_cols = [1,2, 33, 34]      \n",
    "val_cols = [27, 28, 29, 30]\n",
    "labeled_train_cols = [3,4,5,6,7,8 , 35,36,36,37,38]\n",
    "unlabeled_train_cols = list(range(9, 27)) + list(range(40, 44))\n",
    "\n",
    "40,41,42,43,44\n",
    "with open(CSV_PATH, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Epoch', 'Train_Loss', 'Val_Loss', 'Val_mIoU'])\n",
    "\n",
    "print(f\"--- Data Splits ---\")\n",
    "print(f\"Test (Reserved): {test_cols}\")\n",
    "print(f\"Validation: {val_cols}\")\n",
    "print(f\"Labeled Train: {labeled_train_cols}\")\n",
    "print(f\"Unlabeled Train: {unlabeled_train_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6706f",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "Here we define dataloader able to split the $256^3$ into 8 patches of 128^3, which is the largers size we could use, memory limitations. Also the dimensions have the input volumes should be divisable by 8, ensuring that downsampling will yield integer sized feature maps.\n",
    "\n",
    "For the labeled dataset we use data augmentation with gaussian noise and flips. No data augmentation on the unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try:\n",
    "        # 1. Labeled Training Loader\n",
    "        train_dataset = VolumetricPatchDataset(\n",
    "            selected_columns=labeled_train_cols,\n",
    "            augment=True,\n",
    "            is_labeled=True\n",
    "        )\n",
    "        labeled_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "        \n",
    "        # 2. Unlabeled Training Loader\n",
    "        unlabeled_dataset = VolumetricPatchDataset(\n",
    "            selected_columns=unlabeled_train_cols,\n",
    "            augment=False,\n",
    "            is_labeled=False\n",
    "        )\n",
    "        unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "        \n",
    "        # 3. Validation Loader (Labeled, No Augmentation)\n",
    "        val_dataset = VolumetricPatchDataset(\n",
    "            selected_columns=val_cols,\n",
    "            augment=False,\n",
    "            is_labeled=True\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        print(f\"--- Loaders Ready ---\")\n",
    "        print(f\"Train Batches: {len(labeled_loader)}\")\n",
    "        print(f\"Val Batches: {len(val_loader)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating datasets: {e}\")\n",
    "        exit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f636421",
   "metadata": {},
   "source": [
    "### Train - AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f84cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(\n",
    "        in_channels=1, \n",
    "        latent_dim=LATENT_DIM, \n",
    "        NUM_CLASSES=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Loss Functions\n",
    "Tversky = TverskyLoss(num_classes=NUM_CLASSES, alpha=0.6, beta=0.4).to(DEVICE)\n",
    "focal = FocalLoss(gamma=2.0).to(DEVICE)\n",
    "loss_fn_recon = nn.MSELoss().to(DEVICE)\n",
    "\n",
    "loss_fn_seg = ComboLoss(\n",
    "    dice_loss_fn=Tversky,\n",
    "    wce_loss_fn=focal,\n",
    "    alpha=0.4, \n",
    "    beta=0.6   \n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer_model = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.001)\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_model,\n",
    "    mode='max',      # MAXIMIZE IoU\n",
    "    factor=0.5,      \n",
    "    patience=20,     \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "kl_scheduler = KLAnnealing(\n",
    "    start_epoch=0,\n",
    "    end_epoch=50,\n",
    "    start_beta=0.0,\n",
    "    end_beta=0.01)\n",
    "\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "best_val_iou = 0.0\n",
    "patience_counter = 0\n",
    "SAVE_INTERVAL = 20\n",
    "\n",
    "# --- LOSS HISTORY ---\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_iou_history = []\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):        \n",
    "    current_beta = kl_scheduler.get_beta(epoch)\n",
    "    \n",
    "    # --- TRAIN ---\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    epoch_seg_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    \n",
    "    last_x, last_y, last_recon, last_seg = None, None, None, None\n",
    "    for batch_idx, ((x, y_target), x_unlabeled) in \\\n",
    "            enumerate(zip(labeled_loader, itertools.cycle(unlabeled_loader))):\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "        y_target = y_target.to(DEVICE).squeeze(1) # Remove channel dim for loss\n",
    "        x_unlabeled = x_unlabeled.to(DEVICE)\n",
    "\n",
    "        optimizer_model.zero_grad()\n",
    "\n",
    "        # Forward Labeled\n",
    "        seg_out, recon_out, z_mu, z_logvar = model(x)\n",
    "        \n",
    "        # Forward Unlabeled (with noise)\n",
    "        noise = torch.randn_like(x_unlabeled) * 0.1 \n",
    "        _, recon_unlabeled, mu_u, logvar_u = model(x_unlabeled + noise)\n",
    "\n",
    "        # Losses\n",
    "        l_seg = loss_fn_seg(seg_out, y_target)\n",
    "        l_recon = loss_fn_recon(recon_out, x) + loss_fn_recon(recon_unlabeled, x_unlabeled)\n",
    "        \n",
    "        # KL Loss (Average over batch)\n",
    "        l_kld = kld_loss(z_mu, z_logvar) + kld_loss(mu_u, logvar_u)\n",
    "        l_kld = l_kld / (x.size(0) + x_unlabeled.size(0))\n",
    "\n",
    "        total_loss = (l_seg * SEG_WEIGHT) + \\\n",
    "                        (l_recon * RECON_WEIGHT) + \\\n",
    "                        (l_kld * current_beta)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer_model.step()\n",
    "        \n",
    "        train_loss += total_loss.item()\n",
    "        epoch_seg_loss += l_seg.item()\n",
    "        epoch_recon_loss += l_recon.item()\n",
    "\n",
    "        if batch_idx == len(labeled_loader)-1:\n",
    "            last_x = x.detach()\n",
    "            last_y = y_target.detach()\n",
    "            last_recon = recon_out.detach()\n",
    "            last_seg = seg_out.detach()\n",
    "\n",
    "\n",
    "    avg_train_loss = train_loss / len(labeled_loader)\n",
    "    avg_seg_loss = epoch_seg_loss / len(labeled_loader)\n",
    "    avg_recon_loss = epoch_recon_loss / len(labeled_loader)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    \n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    class_inter = np.zeros(NUM_CLASSES)\n",
    "    class_union = np.zeros(NUM_CLASSES)\n",
    "    loss_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x_val, y_val_seg) in val_loader:\n",
    "            x_val = x_val.to(DEVICE)\n",
    "            y_val_seg = y_val_seg.to(DEVICE).squeeze(1).long()\n",
    "            \n",
    "            val_seg_out, _,_,_ = model(x_val)\n",
    "            val_preds = torch.argmax(val_seg_out, dim=1)\n",
    "\n",
    "            loss = loss_fn_seg(val_seg_out, y_val_seg)\n",
    "            loss_val += loss.item()\n",
    "            for c in range(NUM_CLASSES):\n",
    "                pred_c = (val_preds == c)\n",
    "                true_c = (y_val_seg == c)\n",
    "\n",
    "                inter = (pred_c & true_c).sum().item()\n",
    "                union = (pred_c | true_c).sum().item()\n",
    "\n",
    "                class_inter[c] += inter\n",
    "                class_union[c] += union\n",
    "\n",
    "    avg_val_loss = loss_val / len(val_loader)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    class_iou =[]\n",
    "\n",
    "    for c in range(NUM_CLASSES):\n",
    "        if class_union[c] > 0:\n",
    "            iou = class_inter[c]/class_union[c]\n",
    "        else:\n",
    "            iou = 0.0\n",
    "        class_iou.append(iou)\n",
    "    \n",
    "    mIoU = np.mean(class_iou)\n",
    "\n",
    "    val_iou_history.append(mIoU)\n",
    "\n",
    "    with open(CSV_PATH, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch + 1, avg_train_loss, avg_val_loss, mIoU])\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\" Avg Train Loss: {avg_train_loss:.4f} | Seg Loss: {avg_seg_loss:.4f} | Recon Loss: {avg_recon_loss:.4f}\")\n",
    "    print(f\"  Val mIoU: {mIoU:.4f} (Best: {best_val_iou:.4f})\")\n",
    "    print(f\"  [Class IoU] C0: {class_iou[0]} | C1: {class_iou[1]:.4f} | C2: {class_iou[2]:.4f} | C3: {class_iou[3]:.4f}\")\n",
    "    \n",
    "    # Scheduler Step\n",
    "    scheduler.step(mIoU)\n",
    "\n",
    "    # Save Best Model\n",
    "    if mIoU > best_val_iou:\n",
    "        best_val_iou = mIoU\n",
    "        patience_count = 0\n",
    "        torch.save(model.state_dict(), SAVE_PATH)\n",
    "        print(f\"  --> New Best Model Saved!\")\n",
    "    else: \n",
    "        patience_count +=1\n",
    "        (f\"Patience count: {patience_count:.3f}\")\n",
    "\n",
    "    # Visualization\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        print(f\"  Saving visuals for Epoch {epoch+1}...\")\n",
    "        save_predictions(epoch, last_x, last_y, last_recon, last_seg, OUTPUT_DIR)\n",
    "print(\"--- Training Finished ---\")\n",
    "print(\"Saving model weights and curves...\")\n",
    "torch.save(model.state_dict(), SAVE_PATH_FINAL)\n",
    "print(f\"Best model saved {SAVE_PATH}\")\n",
    "print(f\"Final model saved {SAVE_PATH_FINAL}\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f2620",
   "metadata": {},
   "source": [
    "### Model measurements \n",
    "Below are the models measurements and model comparison. From test data\n",
    "\n",
    "> **Table:** Performance of VAE\\_final and VAE\\_best on each input column.\n",
    "\n",
    "| **Model** | **Column** | **ME** | **Mean IoU** | **Mean Dice** | **AUROC** |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: |\n",
    "| VAE\\_final | 1 | 0.0909 | 0.6781 | 0.7158 | 0.8955 |\n",
    "| | 2 | 0.0814 | 0.7354 | 0.8382 | 0.9718 |\n",
    "| | 37 | 0.1617 | 0.7190 | 0.8208 | 0.9609 |\n",
    "| | 38 | 0.0740 | 0.8371 | 0.9084 | 0.9882 |\n",
    "| **Avg** | -- | **0.1020** | **0.7424** | **0.8208** | **0.9541** |\n",
    "| VAE\\_best | 1 | 0.0576 | 0.6888 | 0.7198 | 0.8950 |\n",
    "| | 2 | 0.0698 | 0.7706 | 0.8655 | 0.9754 |\n",
    "| | 37 | 0.1328 | 0.7484 | 0.8451 | 0.9678 |\n",
    "| | 38 | 0.0660 | 0.8599 | 0.9235 | 0.9903 |\n",
    "| **Avg** | -- | **0.0816** | **0.7669** | **0.8385** | **0.9571** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d9765",
   "metadata": {},
   "source": [
    "> **Table 1:** Average performance across all input volumes for VAE models.\n",
    "\n",
    "| **Model** | **ME** | **Mean IoU** | **Mean Dice** | **AUROC** |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| VAE\\_final | 0.1020 | 0.7424 | 0.8208 | 0.9541 |\n",
    "| VAE\\_best | 0.0816 | 0.7669 | 0.8385 | 0.9571 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed9433",
   "metadata": {},
   "source": [
    "> **Table 2:** Per-class IoU and Dice for each input volume (VAE\\_final).\n",
    "\n",
    "| **Column** | C0 IoU | C0 Dice | C1 IoU | C1 Dice | C2 IoU | C2 Dice | C3 IoU | C3 Dice |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | 0.8963 | 0.9453 | 0.9361 | 0.9670 | 0.8694 | 0.9301 | 0.0105 | 0.0208 |\n",
    "| 2 | 0.8088 | 0.8943 | 0.7174 | 0.8355 | 0.9131 | 0.9546 | 0.5021 | 0.6685 |\n",
    "| 37 | 0.7673 | 0.8683 | 0.9164 | 0.9564 | 0.7857 | 0.8800 | 0.4068 | 0.5783 |\n",
    "| 38 | 0.8578 | 0.9234 | 0.9150 | 0.9556 | 0.8952 | 0.9447 | 0.6804 | 0.8098 |\n",
    "\n",
    "\n",
    "> **Table 3:** Per-class IoU and Dice for each input volume (VAE\\_best).\n",
    "\n",
    "| **Column** | C0 IoU | C0 Dice | C1 IoU | C1 Dice | C2 IoU | C2 Dice | C3 IoU | C3 Dice |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | 0.8936 | 0.9438 | 0.9375 | 0.9678 | 0.9191 | 0.9579 | 0.0050 | 0.0099 |\n",
    "| 2 | 0.8204 | 0.9013 | 0.7347 | 0.8470 | 0.9229 | 0.9599 | 0.6047 | 0.7536 |\n",
    "| 37 | 0.7747 | 0.8731 | 0.9174 | 0.9569 | 0.8221 | 0.9024 | 0.4794 | 0.6481 |\n",
    "| 38 | 0.8623 | 0.9261 | 0.9134 | 0.9547 | 0.9019 | 0.9484 | 0.7618 | 0.8648 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc02b2e",
   "metadata": {},
   "source": [
    "> **Table 4:** Comparison of average performance for the final models across all input volumes.\n",
    "\n",
    "| **Model (final)** | **ME** | **Mean IoU** | **Mean Dice** | **AUROC** |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| Multi\\_big\\_final | 0.0985 | 0.7463 | 0.8255 | 0.9686 |\n",
    "| AG\\_final | 0.0623 | 0.8164 | 0.8896 | 0.9886 |\n",
    "| VAE\\_final | 0.1020 | 0.7424 | 0.8208 | 0.9541 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84782ba3",
   "metadata": {},
   "source": [
    "> **Table 5:** Comparison of average performance for the best models across all input volumes.\n",
    "\n",
    "| **Model (best)** | **ME** | **Mean IoU** | **Mean Dice** | **AUROC** |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| Multi\\_big\\_best | 0.0837 | 0.7553 | 0.8306 | 0.9620 |\n",
    "| AG\\_best | 0.0614 | 0.8053 | 0.8759 | 0.9871 |\n",
    "| VAE\\_best | 0.0816 | 0.7669 | 0.8385 | 0.9571 |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
