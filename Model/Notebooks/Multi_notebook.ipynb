{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0723bb",
   "metadata": {},
   "source": [
    "## AE notebook\n",
    "This notebook goes through training steps of the 3D AE model. It may not be possible to run the model in the notebook. Therefore, we refer to multi_big_vali.py and ./bash/multi_big.sh, to test the model yourself.\n",
    "\n",
    "\n",
    "### Important libs and custom functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Custom Imports ---\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent.parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from func.utill import save_predictions, plot_learning_curves\n",
    "from func.loss import DiceLoss, ComboLoss, TverskyLoss, FocalLoss\n",
    "from func.Models import MultiTaskNet_big\n",
    "from func.dataloaders import VolumetricPatchDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c2ab9",
   "metadata": {},
   "source": [
    "### Defining hyperparameters. \n",
    "We also switch to gpu using cuda and save important paths. Splitting data into ~90% train, ~5% validation, ~5% test. Initiation of csv file where we save losses and mIoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49554263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "BLACKHOLE_PATH = os.environ.get('BLACKHOLE', '.')\n",
    "INPUT_SHAPE = (128, 128, 128) \n",
    "NUM_CLASSES = 4\n",
    "LATENT_DIM = 256 \n",
    "BATCH_SIZE = 3 \n",
    "SAVE_INTERVAL = 20\n",
    "NUM_EPOCHS = 400\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "class_weight = torch.tensor([1.0, 1.0, 1.0, 3.0])\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output_big_vali_res\"\n",
    "CSV_PATH = PROJECT_ROOT / \"stats\" / \"training_log_big.csv\"\n",
    "print(f\"Logging metrics to: {CSV_PATH}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_PATH = PROJECT_ROOT / \"Trained_models\" / \"multi_big_res_best.pth\"\n",
    "SAVE_PATH_FINAL = PROJECT_ROOT / \"Trained_models\" / \"multi_big_res_final.pth\"\n",
    "SAVE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- DATA SPLITS ---\n",
    "test_cols = [1,2, 33, 34]      \n",
    "val_cols = [27, 28, 29, 30]\n",
    "labeled_cols = [3,4,5,6,7,8 , 35,36,36,37,38]\n",
    "unlabeled_cols = list(range(9, 27)) + list(range(40, 44))\n",
    "\n",
    "with open(CSV_PATH, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Epoch', 'Train_Loss', 'Val_Loss', 'Val_mIoU'])\n",
    "\n",
    "print(f\"--- Data Splits ---\")\n",
    "print(f\"Labeled Train: {labeled_cols}\")\n",
    "print(f\"Unlabeled Train: {unlabeled_cols}\")\n",
    "print(f\"Validation: {val_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04047b",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "Here we define dataloader able to split the $256^3$ into 8 patches of 128^3, which is the largers size we could use, memory limitations. Also the dimensions have the input volumes should be divisable by 8, ensuring that downsampling will yield integer sized feature maps.\n",
    "\n",
    "For the labeled dataset we use data augmentation with gaussian noise and flips. No data augmentation on the unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfa878",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    labeled_dataset = VolumetricPatchDataset(selected_columns=labeled_cols, augment=True, is_labeled=True)\n",
    "    labeled_loader = DataLoader(labeled_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "\n",
    "    unlabeled_dataset = VolumetricPatchDataset(selected_columns=unlabeled_cols, augment=False, is_labeled=False)\n",
    "    unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "    val_dataset = VolumetricPatchDataset(selected_columns=val_cols, augment=False, is_labeled=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    print(\"--- Loaders Ready ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Datasets: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6afd1",
   "metadata": {},
   "source": [
    "### Train - AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskNet_big(in_channels=1, num_classes=NUM_CLASSES, latent_dim=LATENT_DIM).to(DEVICE)\n",
    "\n",
    "Tversky = TverskyLoss(num_classes=NUM_CLASSES, alpha=0.6, beta=0.4).to(DEVICE)\n",
    "focal = FocalLoss(gamma=2.0, weight=class_weight).to(DEVICE)\n",
    "loss_seg_fn = ComboLoss(dice_loss_fn=Tversky, wce_loss_fn=focal).to(DEVICE)\n",
    "loss_fn_recon = nn.MSELoss().to(DEVICE)\n",
    "\n",
    "optimizer_model = optim.Adam(model.parameters(), lr=1e-4) # Lower LR for stability\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_model, mode=\"max\", factor=0.5, patience=20, verbose=True\n",
    ")\n",
    "\n",
    "best_val_iou = 0.0\n",
    "patience_counter = 0\n",
    "EARLY_STOPPING_PATIENCE = 50\n",
    "\n",
    "# --- LOSS HISTORY ---\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_iou_history = []\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "        \n",
    "    # === TRAINING ===\n",
    "    model.train() \n",
    "    train_loss = 0.0\n",
    "    epoch_seg_loss = 0.0\n",
    "    epoch_recon_loss = 0.0\n",
    "    \n",
    "    last_x, last_y, last_recon, last_seg = None, None, None, None\n",
    "    \n",
    "    for batch_idx, ((x, y_seg_target), x_unlabeled) in \\\n",
    "                    enumerate(zip(labeled_loader, itertools.cycle(unlabeled_loader))):\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "        y_seg_target = y_seg_target.to(DEVICE).squeeze(1)\n",
    "        x_unlabeled = x_unlabeled.to(DEVICE) \n",
    "\n",
    "        optimizer_model.zero_grad()\n",
    "        \n",
    "        # 1. Labeled Forward\n",
    "        seg_out, recon_out_labeled = model(x)\n",
    "        total_loss_seg = loss_seg_fn(seg_out, y_seg_target)\n",
    "        loss_recon_labeled = loss_fn_recon(recon_out_labeled, x)\n",
    "\n",
    "        # 2. Unlabeled Forward\n",
    "        noise = torch.randn_like(x_unlabeled) * 0.1\n",
    "        x_unlabeled_noisy = x_unlabeled + noise\n",
    "        _ , recon_out_unlabeled = model(x_unlabeled_noisy)\n",
    "        \n",
    "        loss_recon_unlabeled = loss_fn_recon(recon_out_unlabeled, x_unlabeled)\n",
    "        \n",
    "        total_loss_recon = loss_recon_labeled + loss_recon_unlabeled\n",
    "        \n",
    "        # Weighted Sum\n",
    "        total_loss = (total_loss_seg * 100.0) + (total_loss_recon * 1.0)\n",
    "            \n",
    "        total_loss.backward()\n",
    "        optimizer_model.step()\n",
    "        \n",
    "        train_loss += total_loss.item()\n",
    "        epoch_seg_loss += total_loss_seg.item()\n",
    "        epoch_recon_loss += total_loss_recon.item()\n",
    "        \n",
    "        if batch_idx == len(labeled_loader) - 1:\n",
    "            last_x = x.detach()\n",
    "            last_y = y_seg_target.detach()\n",
    "            last_recon = recon_out_labeled.detach()\n",
    "            last_seg = seg_out.detach()\n",
    "\n",
    "    avg_train_loss = train_loss / len(labeled_loader)\n",
    "    avg_seg_loss = epoch_seg_loss / len(labeled_loader)\n",
    "    avg_recon_loss = epoch_recon_loss / len(labeled_loader)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    model.eval()\n",
    "    class_inter = np.zeros(NUM_CLASSES)\n",
    "    class_union = np.zeros(NUM_CLASSES)\n",
    "    loss_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vx, vy_seg in val_loader:\n",
    "            vx = vx.to(DEVICE)\n",
    "            vy_seg = vy_seg.to(DEVICE).squeeze(1).long()\n",
    "            \n",
    "            val_seg_out, _ = model(vx)\n",
    "            val_preds = torch.argmax(val_seg_out, dim=1)\n",
    "\n",
    "            loss = loss_seg_fn(val_seg_out, vy_seg)\n",
    "            loss_val += loss.item()\n",
    "            for c in range(NUM_CLASSES):\n",
    "                pred_c = (val_preds == c)\n",
    "                true_c = (vy_seg == c)\n",
    "\n",
    "                inter = (pred_c & true_c).sum().item()\n",
    "                union = (pred_c | true_c).sum().item()\n",
    "\n",
    "                class_inter[c] += inter\n",
    "                class_union[c] += union\n",
    "\n",
    "    avg_val_loss = loss_val / len(val_loader)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    class_iou = []\n",
    "\n",
    "    for c in range(NUM_CLASSES):\n",
    "        if class_union[c] > 0:\n",
    "            iou = class_inter[c] / class_union[c]\n",
    "        else:\n",
    "            iou = 0.0\n",
    "        class_iou.append(iou)\n",
    "    \n",
    "    mIoU = np.mean(class_iou)\n",
    "    val_iou_history.append(mIoU)\n",
    "\n",
    "    with open(CSV_PATH, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch + 1, avg_train_loss, avg_val_loss, mIoU])\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Seg: {avg_seg_loss:.4f} | Recon: {avg_recon_loss:.4f}\")\n",
    "    print(f\"  Val mIoU: {mIoU:.4f} (Best: {best_val_iou:.4f})\")\n",
    "    print(f\"  [Class IoU] C0: {class_iou[0]} | C1: {class_iou[1]:.4f} | C2: {class_iou[2]:.4f} | C3: {class_iou[3]:.4f}\")\n",
    "    \n",
    "    scheduler.step(mIoU)\n",
    "\n",
    "    if mIoU > best_val_iou:\n",
    "        best_val_iou = mIoU\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), SAVE_PATH)\n",
    "        print(f\"  --> New Best Model Saved!\")\n",
    "    else: \n",
    "        patience_counter += 1\n",
    "        print(f\"  Patience count: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        print(f\"  Saving visuals for Epoch {epoch +1}...\")\n",
    "        save_predictions(epoch, last_x, last_y, last_recon, last_seg, OUTPUT_DIR)\n",
    "\n",
    "print(\"--- Training Finished ---\")\n",
    "torch.save(model.state_dict(), SAVE_PATH_FINAL)\n",
    "print(f\"Best model saved {SAVE_PATH}\")\n",
    "print(f\"Final model saved {SAVE_PATH_FINAL}\")\n",
    "print(\"Done.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4df12e",
   "metadata": {},
   "source": [
    "### Model measurements \n",
    "Below are the models measurements and model comparison. From test data\n",
    "> Performance of Multi_big_final and Multi_big_best on each input column\n",
    "\n",
    "| Model             | Column | ME     | Mean IoU | Mean Dice | AUROC  |\n",
    "|------------------|--------|--------|----------|-----------|--------|\n",
    "| **Multi_big_final** | 1      | 0.0824 | 0.6776   | 0.7193    | 0.9392 |\n",
    "|                  | 2      | 0.0770 | 0.7526   | 0.8526    | 0.9759 |\n",
    "|                  | 37     | 0.1629 | 0.7151   | 0.8194    | 0.9683 |\n",
    "|                  | 38     | 0.0717 | 0.8399   | 0.9108    | 0.9911 |\n",
    "| **Avg**          | --     | **0.0985** | **0.7463** | **0.8255** | **0.9686** |\n",
    "| **Multi_big_best**  | 1      | 0.0425 | 0.6917   | 0.7197    | 0.9246 |\n",
    "|                  | 2      | 0.0778 | 0.7498   | 0.8517    | 0.9702 |\n",
    "|                  | 37     | 0.1409 | 0.7330   | 0.8354    | 0.9628 |\n",
    "|                  | 38     | 0.0734 | 0.8468   | 0.9156    | 0.9903 |\n",
    "| **Avg**          | --     | **0.0837** | **0.7553** | **0.8306** | **0.9620** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e83e8",
   "metadata": {},
   "source": [
    "> Average performance across all input volumes for Multi_big models\n",
    "\n",
    "| Model            | ME     | Mean IoU | Mean Dice | AUROC  |\n",
    "|------------------|--------|----------|-----------|--------|\n",
    "| Multi_big_final  | 0.0985 | 0.7463   | 0.8255    | 0.9686 |\n",
    "| Multi_big_best   | 0.0837 | 0.7553   | 0.8306    | 0.9620 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88241635",
   "metadata": {},
   "source": [
    "> Per-class IoU and Dice for each input volume (Multi_big_final)\n",
    "\n",
    "| Column | C0 IoU | C0 Dice | C1 IoU | C1 Dice | C2 IoU | C2 Dice | C3 IoU | C3 Dice |\n",
    "|--------|--------|---------|--------|---------|--------|---------|--------|---------|\n",
    "| 1      | 0.8974 | 0.9459  | 0.9096 | 0.9527  | 0.8826 | 0.9376  | 0.0210 | 0.0411  |\n",
    "| 2      | 0.8054 | 0.8922  | 0.7279 | 0.8425  | 0.9142 | 0.9552  | 0.5630 | 0.7204  |\n",
    "| 37     | 0.7560 | 0.8610  | 0.9079 | 0.9518  | 0.7789 | 0.8757  | 0.4175 | 0.5891  |\n",
    "| 38     | 0.8588 | 0.9240  | 0.9004 | 0.9476  | 0.8976 | 0.9460  | 0.7027 | 0.8254  |\n",
    "\n",
    "> Per-class IoU and Dice for each input volume (Multi_big_best)\n",
    "\n",
    "| Column | C0 IoU | C0 Dice | C1 IoU | C1 Dice | C2 IoU | C2 Dice | C3 IoU | C3 Dice |\n",
    "|--------|--------|---------|--------|---------|--------|---------|--------|---------|\n",
    "| 1      | 0.8951 | 0.9447  | 0.9286 | 0.9630  | 0.9429 | 0.9706  | 0.0002 | 0.0004  |\n",
    "| 2      | 0.7974 | 0.8873  | 0.7080 | 0.8290  | 0.9095 | 0.9526  | 0.5845 | 0.7378  |\n",
    "| 37     | 0.7469 | 0.8551  | 0.9081 | 0.9518  | 0.8009 | 0.8894  | 0.4762 | 0.6452  |\n",
    "| 38     | 0.8524 | 0.9203  | 0.9085 | 0.9521  | 0.8883 | 0.9408  | 0.7381 | 0.8493  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f1f617",
   "metadata": {},
   "source": [
    "> **Table 4:** Comparison of average performance for the final models across all input volumes.\n",
    "\n",
    "| **Model (final)** | **ME** | **Mean IoU** | **Mean Dice** | **AUROC** |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| Multi\\_big\\_final | 0.0985 | 0.7463 | 0.8255 | 0.9686 |\n",
    "| AG\\_final | 0.0623 | 0.8164 | 0.8896 | 0.9886 |\n",
    "| VAE\\_final | 0.1020 | 0.7424 | 0.8208 | 0.9541 |\n",
    "\n",
    "> **Table 5:** Comparison of average performance for the best models across all input volumes.\n",
    "\n",
    "| **Model (best)** | **ME** | **Mean IoU** | **Mean Dice** | **AUROC** |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| Multi\\_big\\_best | 0.0837 | 0.7553 | 0.8306 | 0.9620 |\n",
    "| AG\\_best | 0.0614 | 0.8053 | 0.8759 | 0.9871 |\n",
    "| VAE\\_best | 0.0816 | 0.7669 | 0.8385 | 0.9571 |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
