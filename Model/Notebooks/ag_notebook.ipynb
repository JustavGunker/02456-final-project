{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e87234f",
   "metadata": {},
   "source": [
    "## Attention gated AE notebook\n",
    "This notebook goes through training steps of the attention gated 3D AE. It may not be possible to run the model in the notebook. Therefore, we refer to AG_vali.py and ./bash/AG_run.sh, to test the model yourself.\n",
    "\n",
    "\n",
    "### Important libs and custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56ef0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/zhome/d2/4/167803/Desktop/Deep_project/02456-final-project\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler \n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# --- PROJECT SETUP ---\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(PROJECT_ROOT)\n",
    "\n",
    "from func.utill import save_predictions\n",
    "from func.dataloaders import VolumetricPatchDataset \n",
    "from func.loss import ComboLoss, TverskyLoss, DiceLoss, FocalLoss\n",
    "from func.Models import MultiTaskNet_ag as MultiTaskNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fae87",
   "metadata": {},
   "source": [
    "### Defining hyperparameters. \n",
    "We also switch to gpu using cuda and save important paths. Splitting data into ~90% train, ~5% validation, ~5% test. Initiation of csv file where we save losses and mIoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37918486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using Class Weights: tensor([0.5000, 1.5000, 1.0000, 4.0000])\n",
      "--- Data Splits ---\n",
      "Test (Reserved): [1, 2, 33, 34]\n",
      "Validation: [27, 28, 29, 30]\n",
      "Labeled Train: [3, 4, 5, 6, 7, 8, 35, 36, 36, 37, 38]\n",
      "Unlabeled Train: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (128, 128, 128) \n",
    "NUM_CLASSES = 4\n",
    "LATENT_DIM = 256 \n",
    "BATCH_SIZE = 2\n",
    "SAVE_INTERVAL = 20\n",
    "NUM_EPOCHS = 400\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.001\n",
    "\n",
    "# Weights for Multi-Task Loss\n",
    "SEG_WEIGHT = 100\n",
    "RECON_WEIGHT = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "CLASS_WEIGHTS = torch.tensor([0.5, 1.5, 1.0, 4.0]).to(device) \n",
    "print(f\"Using Class Weights: {CLASS_WEIGHTS}\")\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"Output_AG_vali\"\n",
    "CSV_PATH =  PROJECT_ROOT / \"stats\" / \"training_log_ag.csv\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_PATH_FINAL = PROJECT_ROOT / \"Trained_models\" / \"AG_val_final.pth\"\n",
    "SAVE_PATH = PROJECT_ROOT / \"Trained_models\" / \"AG_val_best.pth\"\n",
    "SAVE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_cols = [1,2, 33, 34]      \n",
    "val_cols = [27, 28, 29, 30]\n",
    "labeled_train_cols = [3,4,5,6,7,8 , 35,36,36,37,38]\n",
    "unlabeled_train_cols = list(range(9, 27)) + list(range(40, 44))\n",
    "\n",
    "with open(CSV_PATH, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Epoch', 'Train_Loss', 'Val_Loss', 'Val_mIoU'])\n",
    "\n",
    "print(f\"--- Data Splits ---\")\n",
    "print(f\"Test (Reserved): {test_cols}\")\n",
    "print(f\"Validation: {val_cols}\")\n",
    "print(f\"Labeled Train: {labeled_train_cols}\")\n",
    "print(f\"Unlabeled Train: {unlabeled_train_cols}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0ee9a",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "Here we define dataloader able to split the $256^3$ into 8 patches of 128^3, which is the largers size we could use, memory limitations. Also the dimensions have the input volumes should be divisable by 8, ensuring that downsampling will yield integer sized feature maps.\n",
    "\n",
    "For the labeled dataset we use data augmentation with gaussian noise and flips. No data augmentation on the unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38656d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Labeled Loader Ready ---\n",
      "--- Unlabeled Loader Ready ---\n",
      "--- Validation Loader Ready ---\n",
      "Train Batches: 88\n",
      "Val Batches: 24\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 1. Labeled Dataset\n",
    "    labeled_dataset = VolumetricPatchDataset(\n",
    "        selected_columns=labeled_train_cols, \n",
    "        augment=True, \n",
    "        is_labeled=True\n",
    "    )\n",
    "    \n",
    "    labeled_loader = DataLoader(\n",
    "        dataset=labeled_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    print(\"--- Labeled Loader Ready ---\")\n",
    "\n",
    "    # 2. Unlabeled Dataset\n",
    "    unlabeled_dataset = VolumetricPatchDataset(\n",
    "        selected_columns=unlabeled_train_cols,\n",
    "        augment=False, \n",
    "        is_labeled=False\n",
    "    )\n",
    "    \n",
    "    unlabeled_loader = DataLoader(\n",
    "        dataset=unlabeled_dataset,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    print(\"--- Unlabeled Loader Ready ---\")\n",
    "\n",
    "    # 3. Validation Loader\n",
    "    val_dataset = VolumetricPatchDataset(\n",
    "        selected_columns=val_cols,\n",
    "        augment=False,\n",
    "        is_labeled=True\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    print(\"--- Validation Loader Ready ---\")\n",
    "    print(f\"Train Batches: {len(labeled_loader)}\")\n",
    "    print(f\"Val Batches: {len(val_loader)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Datasets: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc46f7",
   "metadata": {},
   "source": [
    "### Train - Attention Gated AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskNet(\n",
    "    in_channels=1, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    latent_dim=LATENT_DIM  \n",
    ").to(device)\n",
    "\n",
    "Tversky = TverskyLoss(num_classes=NUM_CLASSES, alpha=0.6, beta=0.4)\n",
    "focal = FocalLoss(gamma=2.0).to(device)\n",
    "loss_fn_recon = nn.MSELoss().to(device)\n",
    "\n",
    "loss_fn_seg = ComboLoss(\n",
    "    dice_loss_fn=Tversky, # normally dice\n",
    "    wce_loss_fn=focal, # normally cross\n",
    "    alpha=0.6, beta=0.4\n",
    ").to(device)\n",
    "\n",
    "optimizer_model = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_model,\n",
    "    mode='max',      \n",
    "    factor=0.5,      \n",
    "    patience=20,     \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best_val_iou = 0.0\n",
    "\n",
    "# --- LOSS HISTORY ---\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_iou_history = []\n",
    "print(\"--- Starting Training ---\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # --- TRAINe ---\n",
    "    model.train() \n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_seg_loss = 0.0\n",
    "    epoch_recon_loss = 0.0\n",
    "    \n",
    "    # Visualization variables\n",
    "    last_x, last_y, last_recon, last_seg = None, None, None, None\n",
    "\n",
    "    for batch_idx, ((x, y_seg_target), (x_unlabeled)) in \\\n",
    "            enumerate(zip(labeled_loader, itertools.cycle(unlabeled_loader))):\n",
    "        \n",
    "        x = x.to(device)       \n",
    "        y_seg_target = y_seg_target.to(device).squeeze(1) # Squeeze for CrossEntropy\n",
    "        x_unlabeled = x_unlabeled.to(device) \n",
    "        \n",
    "        optimizer_model.zero_grad()\n",
    "        \n",
    "        # Forward Labeled\n",
    "        seg_out, recon_out_labeled = model(x)\n",
    "        \n",
    "        # Forward Unlabeled (with noise)\n",
    "        noise = torch.randn_like(x_unlabeled) * 0.1\n",
    "        x_unlabeled_noisy = x_unlabeled + noise\n",
    "        _ , recon_out_unlabeled = model(x_unlabeled_noisy)\n",
    "                    \n",
    "        # Loss Calculation\n",
    "        loss_seg = loss_fn_seg(seg_out, y_seg_target)\n",
    "        loss_recon = loss_fn_recon(recon_out_labeled, x) + \\\n",
    "                        loss_fn_recon(recon_out_unlabeled, x_unlabeled)\n",
    "        \n",
    "        total_loss = (loss_seg*SEG_WEIGHT) + (loss_recon*RECON_WEIGHT)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer_model.step()\n",
    "        \n",
    "        epoch_train_loss += total_loss.item()\n",
    "        epoch_seg_loss += loss_seg.item()\n",
    "        epoch_recon_loss += loss_recon.item()\n",
    "\n",
    "        # Save last batch\n",
    "        if batch_idx == len(labeled_loader) - 1:\n",
    "            last_x = x.detach()\n",
    "            last_y = y_seg_target.detach()\n",
    "            last_recon = recon_out_labeled.detach()\n",
    "            last_seg = seg_out.detach()\n",
    "            \n",
    "    avg_train_loss = epoch_train_loss / len(labeled_loader)\n",
    "    avg_seg_loss = epoch_seg_loss / len(labeled_loader)\n",
    "    avg_recon_loss = epoch_recon_loss / len(labeled_loader)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    \n",
    "    # --- VALIDATION PHASE ---\n",
    "    model.eval()\n",
    "    #total_val_iou = 0.0\n",
    "    class_inter = np.zeros(NUM_CLASSES)\n",
    "    class_union = np.zeros(NUM_CLASSES)\n",
    "    loss_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x_val, y_val_seg) in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val_seg = y_val_seg.to(device).squeeze(1).long()\n",
    "            \n",
    "            val_seg_out, _ = model(x_val)\n",
    "            val_preds = torch.argmax(val_seg_out, dim=1)\n",
    "\n",
    "            loss = loss_fn_seg(val_seg_out, y_val_seg)\n",
    "            loss_val += loss.item()\n",
    "            for c in range(NUM_CLASSES):\n",
    "                pred_c = (val_preds == c)\n",
    "                true_c = (y_val_seg == c)\n",
    "\n",
    "                inter = (pred_c & true_c).sum().item()\n",
    "                union = (pred_c | true_c).sum().item()\n",
    "\n",
    "                class_inter[c] += inter\n",
    "                class_union[c] += union\n",
    "\n",
    "    avg_val_loss = loss_val / len(val_loader)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    class_iou =[]\n",
    "\n",
    "    for c in range(NUM_CLASSES):\n",
    "        if class_union[c] > c:\n",
    "            iou = class_inter[c]/class_union[c]\n",
    "        else:\n",
    "            iou = 0.0\n",
    "        class_iou.append(iou)\n",
    "    \n",
    "    mIoU = np.mean(class_iou)\n",
    "    val_iou_history.append(mIoU)\n",
    "\n",
    "    with open(CSV_PATH, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch + 1, avg_train_loss, avg_val_loss, mIoU])\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\" Avg Train Loss: {avg_train_loss:.4f} | Seg Loss: {avg_seg_loss:.4f} | Recon Loss: {avg_recon_loss:.4f}\")\n",
    "    print(f\"  Val mIoU: {mIoU:.4f} (Best: {best_val_iou:.4f})\")\n",
    "    print(f\" [Class IoU] C0: {class_iou[0]:.4f} C1: {class_iou[1]:.4f} | C2: {class_iou[2]:.4f} | C3: {class_iou[3]:.4f}\")\n",
    "    # Scheduler Step\n",
    "    scheduler.step(mIoU)\n",
    "\n",
    "    # Save Best Model\n",
    "    if mIoU > best_val_iou:\n",
    "        best_val_iou = mIoU\n",
    "        patience_count = 0\n",
    "        torch.save(model.state_dict(), SAVE_PATH)\n",
    "        print(f\"  --> New Best Model Saved!\")\n",
    "    else: \n",
    "        patience_count +=1\n",
    "        (f\"Patience count: {patience_count:.3f}\")\n",
    "\n",
    "    # Visualization\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        print(f\"  Saving visuals for Epoch {epoch+1}...\")\n",
    "        save_predictions(epoch, last_x, last_y, last_recon, last_seg, OUTPUT_DIR)\n",
    "print(\"--- Training Finished ---\")\n",
    "print(\"Saving model weights...\")\n",
    "torch.save(model.state_dict(), SAVE_PATH_FINAL)\n",
    "print(f\"Best model saved {SAVE_PATH}\")\n",
    "print(f\"Final model saved {SAVE_PATH_FINAL}\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e647089",
   "metadata": {},
   "source": [
    "## Model measurements\n",
    "Below are the models measurements and model comparison. From test data\n",
    "> Performance of AG_final and AG_best on each input column\n",
    "\n",
    "| Model       | Column | ME     | Mean IoU | Mean Dice | AUROC  |\n",
    "|-------------|--------|--------|----------|-----------|--------|\n",
    "| **AG_final** | 1      | 0.0295 | 0.7969   | 0.8610    | 0.9946 |\n",
    "|             | 2      | 0.0682 | 0.7787   | 0.8717    | 0.9860 |\n",
    "|             | 37     | 0.0939 | 0.8026   | 0.8857    | 0.9801 |\n",
    "|             | 38     | 0.0576 | 0.8874   | 0.9401    | 0.9938 |\n",
    "| **Avg**     | --     | **0.0623** | **0.8164** | **0.8896** | **0.9886** |\n",
    "| **AG_best**  | 1      | 0.0333 | 0.7487   | 0.8044    | 0.9883 |\n",
    "|             | 2      | 0.0692 | 0.7704   | 0.8656    | 0.9839 |\n",
    "|             | 37     | 0.0888 | 0.8097   | 0.8905    | 0.9822 |\n",
    "|             | 38     | 0.0543 | 0.8924   | 0.9429    | 0.9939 |\n",
    "| **Avg**     | --     | **0.0614** | **0.8053** | **0.8759** | **0.9871** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49c85b",
   "metadata": {},
   "source": [
    "> Average performance across all input volumes for AG models\n",
    "\n",
    "| Model     | ME     | Mean IoU | Mean Dice | AUROC  |\n",
    "|-----------|--------|----------|-----------|--------|\n",
    "| AG_final  | 0.0623 | 0.8164   | 0.8896    | 0.9886 |\n",
    "| AG_best   | 0.0614 | 0.8053   | 0.8759    | 0.9871 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a65d7",
   "metadata": {},
   "source": [
    "> Per-class IoU and Dice for each input volume (AG_final)\n",
    "\n",
    "| Column | C0 IoU | C0 Dice | C1 IoU | C1 Dice | C2 IoU | C2 Dice | C3 IoU | C3 Dice |\n",
    "|--------|--------|---------|--------|---------|--------|---------|--------|---------|\n",
    "| 1      | 0.9238 | 0.9604  | 0.9373 | 0.9676  | 0.9600 | 0.9796  | 0.3666 | 0.5365  |\n",
    "| 2      | 0.8095 | 0.8947  | 0.7526 | 0.8589  | 0.9216 | 0.9592  | 0.6312 | 0.7739  |\n",
    "| 37     | 0.7933 | 0.8847  | 0.9271 | 0.9622  | 0.8710 | 0.9310  | 0.6191 | 0.7647  |\n",
    "| 38     | 0.8690 | 0.9299  | 0.9177 | 0.9571  | 0.9092 | 0.9525  | 0.8538 | 0.9211  |\n",
    "\n",
    "> Per-class IoU and Dice for each input volume (AG_best)\n",
    "\n",
    "| Column | C0 IoU | C0 Dice | C1 IoU | C1 Dice | C2 IoU | C2 Dice | C3 IoU | C3 Dice |\n",
    "|--------|--------|---------|--------|---------|--------|---------|--------|---------|\n",
    "| 1      | 0.9141 | 0.9551  | 0.9357 | 0.9668  | 0.9556 | 0.9773  | 0.1893 | 0.3183  |\n",
    "| 2      | 0.8112 | 0.8958  | 0.7390 | 0.8499  | 0.9218 | 0.9592  | 0.6098 | 0.7576  |\n",
    "| 37     | 0.8074 | 0.8935  | 0.9230 | 0.9600  | 0.8771 | 0.9345  | 0.6314 | 0.7741  |\n",
    "| 38     | 0.8784 | 0.9353  | 0.9200 | 0.9584  | 0.9133 | 0.9547  | 0.8577 | 0.9234  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbe9e0",
   "metadata": {},
   "source": [
    "> **Table 4:** Comparison of average performance for the final models across all input volumes.\n",
    "\n",
    "| **Model (final)** | **ME** | **Mean IoU** | **Mean Dice** | **AUROC** |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| Multi\\_big\\_final | 0.0985 | 0.7463 | 0.8255 | 0.9686 |\n",
    "| AG\\_final | 0.0623 | 0.8164 | 0.8896 | 0.9886 |\n",
    "| VAE\\_final | 0.1020 | 0.7424 | 0.8208 | 0.9541 |\n",
    "\n",
    "> **Table 5:** Comparison of average performance for the best models across all input volumes.\n",
    "\n",
    "| **Model (best)** | **ME** | **Mean IoU** | **Mean Dice** | **AUROC** |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| Multi\\_big\\_best | 0.0837 | 0.7553 | 0.8306 | 0.9620 |\n",
    "| AG\\_best | 0.0614 | 0.8053 | 0.8759 | 0.9871 |\n",
    "| VAE\\_best | 0.0816 | 0.7669 | 0.8385 | 0.9571 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MBML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
