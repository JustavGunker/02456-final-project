{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9311186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from IPython.display import Image, display, clear_output\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import Tensor\n",
    "from torch.distributions import Normal\n",
    "from torchvision.utils import make_grid\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320a575a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 172\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# Test with the input size from your diagram (572x572)\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mUnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_hid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid_hid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Input is 1 sample, 1 channel, 572x572\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     test_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m572\u001b[39m, \u001b[38;5;241m572\u001b[39m) \n",
      "Cell \u001b[0;32mIn[4], line 154\u001b[0m, in \u001b[0;36mUnet.__init__\u001b[0;34m(self, num_classes, in_hid, hid_hid)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m Encoder()\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottleneck \u001b[38;5;241m=\u001b[39m \u001b[43mBottleneck\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_hid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid_hid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m Decoder(num_classes)\n",
      "Cell \u001b[0;32mIn[4], line 109\u001b[0m, in \u001b[0;36mBottleneck.__init__\u001b[0;34m(self, input_dim, z_dim)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, z_dim):\n\u001b[0;32m--> 109\u001b[0m     C, H, W \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mFlatten()\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(C \u001b[38;5;241m*\u001b[39m H \u001b[38;5;241m*\u001b[39m W, z_dim)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def double_conv_unpadded(in_channels, out_channels):\n",
    "    \"\"\"\n",
    "    Two consecutive 3x3 unpadded convolutions (padding=0).\n",
    "    Total size reduction per block is 4 pixels (2 from first conv + 2 from second conv).\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net Encoder step: Conv-Conv block followed by Max Pooling.\n",
    "    Returns both the feature map (for skip) and the pooled map (for next block).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Conv block\n",
    "        self.conv = double_conv_unpadded(in_channels, out_channels)\n",
    "        # Pooling layer \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_output = self.conv(x)\n",
    "        pool_output = self.pool(conv_output)\n",
    "        return conv_output, pool_output\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net Decoder step: Up-convolution, Cropping, Concatenation, and Refinement.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.up = nn.ConvTranspose2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=2, \n",
    "            stride=2\n",
    "        )\n",
    "        self.conv = double_conv_unpadded(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_bottom, x_skip):\n",
    "        \n",
    "        # Double the size of the feature map from the layer below\n",
    "        x_up = self.up(x_bottom) \n",
    "        \n",
    "        # The skip connection (x_skip) is LARGER than the upsampled map (x_up) in this architecture.\n",
    "        # We must calculate the difference and crop the larger x_skip to match the smaller x_up.\n",
    "        diff_h = x_skip.size(2) - x_up.size(2)\n",
    "        diff_w = x_skip.size(3) - x_up.size(3)\n",
    "        \n",
    "        # ensure the sizes match\n",
    "        if diff_h < 0 or diff_w < 0:\n",
    "             raise RuntimeError(f\"Cropping error: x_up ({x_up.shape}) is unexpectedly larger than x_skip ({x_skip.shape}). Check encoder padding.\")\n",
    "\n",
    "        # apply croping to the larger x_skip tensor to match x_up size\n",
    "        x_skip_cropped = x_skip[:, :, \n",
    "            diff_h // 2 : x_skip.size(2) - diff_h + diff_h // 2, \n",
    "            diff_w // 2 : x_skip.size(3) - diff_w + diff_w // 2\n",
    "        ]\n",
    "        \n",
    "        # concatenate the x_up with skipped conection\n",
    "        x_combined = torch.cat([x_up, x_skip_cropped], dim=1)\n",
    "        \n",
    "        # Pass combined map through the convolutional block\n",
    "        return self.conv(x_combined)\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Block-based U-Net Encoder: 5 levels of downsampling.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # blocks \n",
    "        self.block1 = EncoderBlock(1, 64)        # 1 -> 64 channels\n",
    "        self.block2 = EncoderBlock(64, 128)      # 64 -> 128 channels\n",
    "        self.block3 = EncoderBlock(128, 256)     # 128 -> 256 channels\n",
    "        self.block4 = EncoderBlock(256, 512)     # 256 -> 512 channels\n",
    "        \n",
    "        # bttleneck \n",
    "        self.bottleneck = double_conv_unpadded(512, 1024) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, p1 = self.block1(x)\n",
    "        x2, p2 = self.block2(p1)\n",
    "        x3, p3 = self.block3(p2)\n",
    "        x4, p4 = self.block4(p3)\n",
    "        x5 = self.bottleneck(p4)\n",
    "        \n",
    "        return x1, x2, x3, x4, x5\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, input_dim, z_dim):\n",
    "        C, H, W = input_dim\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(C * H * W, z_dim)\n",
    "        self.af1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(z_dim, C*H*W)\n",
    "        self.C, self.H, self.W = C, H, W\n",
    "    def forward(self, x):\n",
    "        x_flat = self.flatten(x)\n",
    "        x = self.fc1(x_flat)\n",
    "        x = self.af1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_classes=1): \n",
    "        super().__init__()\n",
    "        \n",
    "        # 1024 -> 512 (combines with Encoder x4, 512 channels)\n",
    "        self.upconv4 = DecoderBlock(in_channels=1024, skip_channels=512, out_channels=512) \n",
    "        \n",
    "        # 512 -> 256 (Combines with Encoder x3, 256 channels)\n",
    "        self.upconv3 = DecoderBlock(in_channels=512, skip_channels=256, out_channels=256) \n",
    "        \n",
    "        # 256 -> 128 (Combines with Encoder x2, 128 channels)\n",
    "        self.upconv2 = DecoderBlock(in_channels=256, skip_channels=128, out_channels=128) \n",
    "        \n",
    "        # 128 -> 64 (Combines with Encoder x1, 64 channels)\n",
    "        self.upconv1 = DecoderBlock(in_channels=128, skip_channels=64, out_channels=64) \n",
    "        \n",
    "        self.out_conv = nn.Conv2d(64, num_classes, kernel_size=1) \n",
    "\n",
    "    def forward(self, x5_bottleneck, x4_skip, x3_skip, x2_skip, x1_skip):\n",
    "        d4 = self.upconv4(x5_bottleneck, x4_skip)\n",
    "        d3 = self.upconv3(d4, x3_skip)\n",
    "        d2 = self.upconv2(d3, x2_skip)\n",
    "        d1 = self.upconv1(d2, x1_skip)\n",
    "        \n",
    "        return self.out_conv(d1)\n",
    "\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, num_classes=1, in_hid=1, hid_hid=1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.bottleneck = Bottleneck(in_hid, hid_hid)\n",
    "        self.decoder = Decoder(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1, x2, x3, x4, x5 = self.encoder(x)\n",
    "        print(x5.shape, x4.shape)\n",
    "        x5 = self.bottleneck(x5)\n",
    "        \n",
    "        # Decoder\n",
    "        output = self.decoder(x5, x4, x3, x2, x1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Test with the input size from your diagram (572x572)\n",
    "    model = Unet(num_classes=2, in_hid = 28*28, hid_hid=1024) \n",
    "    \n",
    "    # Input is 1 sample, 1 channel, 572x572\n",
    "    test_input = torch.randn(1, 1, 572, 572) \n",
    "    \n",
    "    # Run the model\n",
    "    output = model(test_input)\n",
    "    \n",
    "    # Expected output size (original size - (4 pixels * 4 downsampling steps) = 572 - 16 = 556? No, cropping handles this)\n",
    "    # The output size should match the size after the last unpadded double conv:\n",
    "    # 572 -> 568 (x1)\n",
    "    # The decoder will align the final output to this size.\n",
    "    print(f\"Unet Output shape: {list(output.shape)}\") # Expected: [1, 2, 568, 568]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train_loader, test_laoder, and valid_loader.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cbef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(lr=lr)\n",
    "criterion_l = nn.MSELoss()\n",
    "criterion_u = nn.CrossEntropyLoss\n",
    "valid_loss = []\n",
    "train_loss = []\n",
    "    \n",
    "Unet.train()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    batch_loss = []\n",
    "    for x_l, y_l in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        x = input # some kind of data splittin here \n",
    "        output = Unet(x)\n",
    "\n",
    "\n",
    "\n",
    "        loss_l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss.append(loss)\n",
    "        \n",
    "        loss_\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MBML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
