{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f50ce5f",
   "metadata": {},
   "source": [
    "### Semi-supervised learning\n",
    "## Multi-tasking \n",
    "Data taken from https://www.kaggle.com/datasets/prathamgrover/3d-liver-segmentation?resource=download'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cef25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "cd= Path.cwd().parent\n",
    "sys.path.append(str(cd))\n",
    "\n",
    "from func.utill import visualize_slices, DiceLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff699e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check size of all nii files in a folder\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def check_nii_sizes(folder_path):\n",
    "    nii_files = glob.glob(os.path.join(folder_path, \"*.nii\"))\n",
    "    for file in nii_files:\n",
    "        img = nib.load(file)\n",
    "        data = img.get_fdata()\n",
    "        print(f\"File: {os.path.basename(file)}, Shape: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03176e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_SHAPE = (40, 40, 40) # ( D, H, W)\n",
    "NUM_CLASSES = 3  # Background, Segment 1, Segment 2\n",
    "LATENT_DIM = 256 # RNN batch\n",
    "BATCH_SIZE = 4\n",
    "TIME_STEPS = 10 # Time series size \n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3D Convolutional block: Conv3D -> BatchNorm -> ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Seg_decoder(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.up_seg1 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2) # -> 64x14x14x14\n",
    "        self.dec_seg1 = ConvBlock(128, 64) # add skip connection 64+64= 128\n",
    "        \n",
    "        self.up_seg2 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2) # 32x28x28x28\n",
    "        self.dec_seg2 = ConvBlock(64, 32) # add skip connection 32+32= 64\n",
    "        \n",
    "        self.out_seg = nn.Conv3d(32, num_classes, kernel_size=1) # Output segmentation\n",
    "        \n",
    "    def forward(self, b , s1, s2):\n",
    "         # segmentation decoder forward step\n",
    "        us1 = self.up_seg1(b) # -> Bx64x14x14x14\n",
    "        ds1 = self.dec_seg1(torch.cat([us1, s2], dim=1)) # Concat skip 2\n",
    "        \n",
    "        us2 = self.up_seg2(ds1) # -> Bx32x28x28x28\n",
    "        ds2 = self.dec_seg2(torch.cat([us2, s1], dim=1)) # Concat skip 1\n",
    "        us3 = self.out_seg(ds2) \n",
    "        return us3\n",
    "    \n",
    "class Recon_decoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.up_recon1 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2) # ->64x14x14x14\n",
    "        self.dec_recon1 = ConvBlock(128, 64)\n",
    "\n",
    "        self.up_recon2 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2) # -> 32x28x28x28\n",
    "        self.dec_recon2 = ConvBlock(64, 32)\n",
    "\n",
    "        self.out_recon = nn.Sequential(\n",
    "            nn.Conv3d(32, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, b, s1, s2):\n",
    "        ur1 = self.up_recon1(b)   # -> Bx64x14x14x14\n",
    "        dr1 = self.dec_recon1(torch.cat([ur1, s2], dim=1))\n",
    "        \n",
    "        ur2 = self.up_recon2(dr1) # -> Bx3x28x28x28\n",
    "        dr2 = self.dec_recon2(torch.cat([ur2, s1], dim=1))\n",
    "\n",
    "        ur3 = self.out_recon(dr2) # -> Bx1x28x28x28\n",
    "\n",
    "        return ur3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d40d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiTaskNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=3, latent_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Commen encoder\n",
    "        # 1x28x28x28\n",
    "        self.enc1 = ConvBlock(in_channels, 32) # -> 32x28x28x28\n",
    "        self.pool1 = nn.MaxPool3d(2) # -> 32x14x14x14\n",
    "        \n",
    "        self.enc2 = ConvBlock(32, 64) # -> 64x14x14x14\n",
    "        self.pool2 = nn.MaxPool3d(2) # -> 64x7x7x7\n",
    "        \n",
    "        # Bottleneck \n",
    "        self.bottleneck = ConvBlock(64, 128) # -> 128x7x7x7\n",
    "        \n",
    "        # Feature vector for rnn input\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.to_latent_vec = nn.Linear(128, latent_dim) # -> Bx256x\n",
    "\n",
    "        # First decoder head for segmentation with skipped connect\n",
    "        self.seg_decoder = Seg_decoder(num_classes=num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Second decoder head reconstruction without skipped\n",
    "        self.recon_decoder = Recon_decoder(in_channels=in_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # commen encoder\n",
    "        s1 = self.enc1(x)       # -> Bx32x28x28x28]\n",
    "        p1 = self.pool1(s1)     # -> Bx32x14x14x14\n",
    "        \n",
    "        s2 = self.enc2(p1)      # -> Bx64x14x14x14\n",
    "        p2 = self.pool2(s2)     # -> Bx64x7x7x7x\n",
    "        \n",
    "        # bottleneck -> could be variational\n",
    "        b = self.bottleneck(p2) # [B, 128, 7, 7, 7]\n",
    "\n",
    "        # Vectorize bottleneck output \n",
    "        pooled_vec = self.global_pool(b).view(b.size(0), -1) # -<Bx128\n",
    "        latent_z = self.to_latent_vec(pooled_vec)            # ->Bx256\n",
    "\n",
    "        # Segmentation decoder head with skips\n",
    "        seg_output = self.seg_decoder(b, s1, s2)\n",
    "        seg_output = self.sigmoid(seg_output)\n",
    "\n",
    "        # Reconstruction decoder head without skips\n",
    "        recon_output = self.recon_decoder(b, s1, s2)\n",
    "        \n",
    "        return seg_output, recon_output, latent_z\n",
    "\n",
    "\n",
    "# Temporal model -> could add more dimension \n",
    "class TemporalTracker(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True \n",
    "        )\n",
    "\n",
    "        # predict t+1\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, z_sequence):\n",
    "        # z_sequence shape -> Batch x Time_Steps x latent_dim\n",
    "        lstm_out, _ = self.lstm(z_sequence)\n",
    "        \n",
    "        # only care about t+1\n",
    "        last_step_out = lstm_out[:, -1, :] # -1 last time step +1 \n",
    "        \n",
    "        # fc layer for prediction\n",
    "        prediction = self.fc(last_step_out)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiverDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Made by AI\n",
    "    Custom PyTorch Dataset for the 3D Liver Segmentation data.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, label_dir, target_size=INPUT_SHAPE):\n",
    "        print(image_dir)\n",
    "        print(label_dir)\n",
    "        # --- THIS IS THE CORRECTED PART (looking for .nii) ---\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"imagesTr\",\"*.nii\")))\n",
    "        self.label_paths = sorted(glob.glob(os.path.join(label_dir, \"labelsTr\" , \"*.nii\")))      \n",
    "        self.target_size = target_size # (D, H, W)\n",
    "        \n",
    "        # Ensure we have matched pairs\n",
    "        assert len(self.image_paths) > 0, f\"No images found in {image_dir}\"\n",
    "        assert len(self.label_paths) > 0, f\"No labels found in {label_dir}\"\n",
    "        assert len(self.image_paths) == len(self.label_paths), \\\n",
    "            f\"Found {len(self.image_paths)} images but {len(self.label_paths)} labels.\"\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} image/label pairs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def normalize(self, data):\n",
    "        # Normalize pixel values to [0, 1]\n",
    "        data = data - torch.min(data)\n",
    "        data = data / torch.max(data)\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Load NIfTI files (nibabel handles .nii and .nii.gz the same way)\n",
    "        img_nii = nib.load(self.image_paths[idx])\n",
    "        lbl_nii = nib.load(self.label_paths[idx])\n",
    "        \n",
    "        # 2. Get data as numpy array and convert to tensor\n",
    "        img_tensor = torch.from_numpy(img_nii.get_fdata()).float().permute(2, 1, 0).unsqueeze(0)\n",
    "        lbl_tensor = torch.from_numpy(lbl_nii.get_fdata()).long().permute(2, 1, 0).unsqueeze(0)\n",
    "\n",
    "        # 3. Resize\n",
    "        img_resized = F.interpolate(img_tensor.unsqueeze(0), \n",
    "                                    size=self.target_size, \n",
    "                                    mode='trilinear', \n",
    "                                    align_corners=False).squeeze(0)\n",
    "        \n",
    "        lbl_resized = F.interpolate(lbl_tensor.float().unsqueeze(0), \n",
    "                                    size=self.target_size, \n",
    "                                    mode='nearest').squeeze(0).long()\n",
    "\n",
    "        # 4. Normalize image\n",
    "        img_resized = self.normalize(img_resized)\n",
    "\n",
    "        # Squeeze the channel dim from the label\n",
    "        lbl_resized = lbl_resized.squeeze(0) \n",
    "\n",
    "        return img_resized, lbl_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiverUnlabeledDataset(Dataset):\n",
    "    \"\"\"\n",
    "    made by AI\n",
    "    Custom PyTorch Dataset for 3D Liver UNLABELED images.\n",
    "    Loads only images and returns them as a 1-item tuple.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, target_size=INPUT_SHAPE, subfolder=\"imagesTr\"):\n",
    "        # Assumes unlabeled images are in a folder like 'imagesUnlabeledTr'\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, subfolder, \"*.nii\")))\n",
    "        self.target_size = target_size # (D, H, W)\n",
    "        \n",
    "        assert len(self.image_paths) > 0, f\"No unlabeled images found in {os.path.join(image_dir, subfolder)}\"\n",
    "        print(f\"Found {len(self.image_paths)} unlabeled images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def normalize(self, data):\n",
    "        # Normalize pixel values to [0, 1]\n",
    "        data = data - torch.min(data)\n",
    "        data = data / torch.max(data)\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Load NIfTI file\n",
    "        img_nii = nib.load(self.image_paths[idx])\n",
    "        \n",
    "        # 2. Get data as numpy array and convert to tensor\n",
    "        img_tensor = torch.from_numpy(img_nii.get_fdata()).float().permute(2, 1, 0).unsqueeze(0)\n",
    "\n",
    "        # 3. Resize\n",
    "        img_resized = F.interpolate(img_tensor.unsqueeze(0), \n",
    "                                    size=self.target_size, \n",
    "                                    mode='trilinear', \n",
    "                                    align_corners=False).squeeze(0)\n",
    "        \n",
    "        # 4. Normalize image\n",
    "        img_resized = self.normalize(img_resized)\n",
    "\n",
    "        # 5. Return as a 1-item tuple\n",
    "        # This is important so the loop `(x_unlabeled)` unpacks correctly\n",
    "        return (img_resized,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "DATA_DIR = \"./Task03_Liver_rs\" \n",
    "# This one path points to the root directory (e.g., ./Task03_Liver_rs)\n",
    "data_root_folder = Path.cwd().parent / DATA_DIR\n",
    "\n",
    "\n",
    "try:\n",
    "    # labeled set\n",
    "    labeled_dataset = LiverDataset(image_dir=data_root_folder, label_dir=data_root_folder, target_size= INPUT_SHAPE)\n",
    "    \n",
    "    #DataLoader for labeled data\n",
    "    labeled_loader = DataLoader(\n",
    "        dataset=labeled_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "\n",
    "        shuffle=True\n",
    "    )\n",
    "    print(\"--- success ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Labeled dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "\n",
    "    unlabeled_dataset = LiverUnlabeledDataset(\n",
    "        image_dir=data_root_folder, \n",
    "        subfolder=\"imagesUnlabelledTr\",\n",
    "        target_size= INPUT_SHAPE\n",
    "    )\n",
    "    \n",
    "    # \n",
    "    unlabeled_loader = DataLoader(\n",
    "        dataset=unlabeled_dataset,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True\n",
    "    )\n",
    "    print(\"--- success ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Unlabeled dataset: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4309ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # start model\n",
    "    model = MultiTaskNet(\n",
    "        in_channels=1, \n",
    "        num_classes=NUM_CLASSES, \n",
    "        latent_dim=LATENT_DIM  \n",
    "    ).to(device)\n",
    "\n",
    "    # define loss and optimizer\n",
    "    loss_fn_seg_dice = DiceLoss(num_classes= NUM_CLASSES)\n",
    "    loss_fn_seg_cross = nn.CrossEntropyLoss()\n",
    "    loss_fn_recon = nn.MSELoss()\n",
    "    optimizer_model = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    print(\"--- Training the MultiTaskNet on Liver Data ---\")\n",
    "    NUM_EPOCHS = 31\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "        \n",
    "        model.train() \n",
    "        \n",
    "        # iterate over both loaders\n",
    "        for batch_idx, ((x_labeled, y_seg_target), (x_unlabeled)) in \\\n",
    "                enumerate(zip(labeled_loader, itertools.cycle(unlabeled_loader))):\n",
    "            \n",
    "            # Move all data to device\n",
    "            x_labeled = x_labeled.to(device)\n",
    "            y_seg_target = y_seg_target\n",
    "            x_unlabeled = x_unlabeled\n",
    "            x_unlabeled = x_unlabeled[0]\n",
    "\n",
    "            optimizer_model.zero_grad()\n",
    "            \n",
    "            # forward pass on labeled data for both seg and recon\n",
    "            seg_out, recon_out_labeled, _ = model(x_labeled)\n",
    "            \n",
    "            # segmentation losses\n",
    "            loss_seg_cross = loss_fn_seg_cross(seg_out, y_seg_target)\n",
    "            \n",
    "            total_loss_seg = loss_seg_cross * 1.0\n",
    "            loss_seg_dice = 0.0\n",
    "            \n",
    "            # add dice loss if cross entropy is low enough\n",
    "            if loss_seg_cross.item() < 0.6:\n",
    "                dice_loss_component = loss_fn_seg_dice(seg_out, y_seg_target)\n",
    "                total_loss_seg = total_loss_seg + (dice_loss_component * 1)\n",
    "                loss_seg_dice = dice_loss_component.item()\n",
    "                \n",
    "            # labeled recon loss\n",
    "            loss_recon_labeled = loss_fn_recon(recon_out_labeled, x_labeled)\n",
    "                \n",
    "            # Forward pass only on unlabeled data for recon\n",
    "            _ , recon_out_unlabeled, _ = model(x_unlabeled)\n",
    "            \n",
    "            # unlabeled recon loss\n",
    "            loss_recon_unlabeled = loss_fn_recon(recon_out_unlabeled, x_unlabeled)\n",
    "            \n",
    "            # Total recon loss\n",
    "            total_loss_recon = loss_recon_labeled + loss_recon_unlabeled\n",
    "            \n",
    "            # Total combined loss\n",
    "            total_loss = total_loss_seg + (total_loss_recon * 0.5) \n",
    "                \n",
    "            total_loss.backward()\n",
    "            optimizer_model.step()\n",
    "            \n",
    "            # Udate Logging\n",
    "            if batch_idx % 30 == 0:\n",
    "                if loss_seg_cross.item() > 0.6:\n",
    "                    print(f\"Batch {batch_idx}/{len(labeled_loader)} | Total Loss: {total_loss.item():.4f} | Recon Loss (Total): {total_loss_recon.item():.4f} | CE Loss (Labeled): {loss_seg_cross.item():.4f} (Dice not active)\")\n",
    "                else:\n",
    "                    print(f\"Batch {batch_idx}/{len(labeled_loader)} | Total Loss: {total_loss.item():.4f} | Recon Loss (Total): {total_loss_recon.item():.4f} | CE Loss (Labeled): {loss_seg_cross.item():.4f} | DICE Loss (Labeled): {loss_seg_dice:.4f}\")\n",
    "            \n",
    "            # visualization update\n",
    "            if epoch % 10 == 0 and batch_idx % 30 == 0:\n",
    "                print(\"--- Visualizing first training batch (Labeled Data) ---\")\n",
    "                visualize_slices(x_labeled, y_seg_target, recon_out_labeled, seg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_slices(x_labeled, y_seg_target, recon_out_labeled, seg_out, slice_idx=13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MBML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
